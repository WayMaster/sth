{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from encoder_new import encoder\n",
    "from decoder_new import decoder\n",
    "import time\n",
    "\n",
    "SOS = 0\n",
    "EOS = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "输入encoder—input和batch-size和模式，输出暂时未知\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices(np.random.uniform(size=(5, 2))) \n",
    "传入的数值是一个矩阵，它的形状为(5, 2)，tf.data.Dataset.from_tensor_slices就会切分它形状上的第一个维度，最后生成的dataset中一个含有5个元素，每个元素的形状是(2, )，即每个元素是矩阵的一行。\n",
    "放到这里，encoder_input是一个list，里面每一个元素都代表了一个架构，因此可以看成切成了一个个架构\n",
    "\n",
    "该函数是用来对输入进行一个预处理的函数\n",
    "\n",
    "one_shot_iterator 返回的是一个iterator，并且数据集只能从头到尾读一次\n",
    "\n",
    "目前只完成了这个函数一半的考察\n",
    "\n",
    "接下来开始补mode为train时的部分其中symmetry暂时不知道干啥的，encoder-target是一个关于encoderinput对应架构精度的一个东西，decoder-target是encoderinput的拷贝\n",
    "\n",
    "preprocess函数用来增加decoder_input, 这个decoder—input具体怎么用，未知，只知道一开始插入了一个起始标记SOS，感觉像循环神经网络的每次的input，然后输出下一个code，细节不太清楚，也不是清楚为什么可以用tf.concat([sos_id, decoder_tgt[:-1],axis=0)这样连起来，sos_id不是就只有一个0吗难道。原来decoder_tgt[:-1]现在也就是一个一维的张量而已。这就解释了为什么可以concat。但是如何处理这个decoder_input还得从后序代码中理解\n",
    "\n",
    "generate_symmetry是根据一个结构生成镜像结构是函数，详细说，操作就是随机挑出某一个(I1,OP1,I2,OP2)转换成(I2,OP2,I1,OP1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_fn(encoder_input, encoder_target, decoder_target, mode, batch_size, num_epochs=1, symmetry=False):\n",
    "    shape = np.array(encoder_input).shape\n",
    "    if mode == 'train':\n",
    "        tf.logging.info('Data size: {}, {}'.format(shape, np.array(encoder_target).shape))\n",
    "        N = shape[0]\n",
    "        source_length = shape[1]\n",
    "        encoder_input = tf.convert_to_tensor(encoder_input, dtype=tf.int32)\n",
    "        encoder_input = tf.data.Dataset.from_tensor_slices(encoder_input)\n",
    "        encoder_target = tf.convert_to_tensor(encoder_target, dtype=tf.float32)\n",
    "        encoder_target = tf.data.Dataset.from_tensor_slices(encoder_target)\n",
    "        decoder_target = tf.convert_to_tensor(decoder_target, dtype=tf.int32)\n",
    "        decoder_target = tf.data.Dataset.from_tensor_slices(decoder_target)\n",
    "        dataset = tf.data.Dataset.zip((encoder_input, encoder_target, decoder_target))\n",
    "        dataset = dataset.shuffle(buffer_size=N)\n",
    "        \n",
    "        def preprocess(encoder_src, encoder_tgt, decoder_tgt): #src:sequence tgt:performance\n",
    "            sos_id = tf.constant([SOS])\n",
    "            decoder_src = tf.concat([sos_id, decoder_tgt[:-1]], axis=0)\n",
    "            return (encoder_src, encoder_tgt, decoder_src, decoder_tgt)\n",
    "        def generate_symmetry(encoder_src, encoder_tgt, decoder_src, decoder_tgt):\n",
    "            a = tf.random_uniform([], 0, 5, dtype=tf.int32)\n",
    "            b = tf.random_uniform([], 0, 5, dtype=tf.int32)\n",
    "            cell_seq_length = source_length // 2\n",
    "            assert source_length in [40, 60]\n",
    "            if source_length == 40:\n",
    "                encoder_src = tf.concat([encoder_src[:4 * a], encoder_src[4*a+2 : 4*a+4], encoder_src[4*a:4*a+2], encoder_src[4*(a+1):cell_seq_length+4*b], encoder_src[cell_seq_length+4*b+2:cell_seq_length+4*b+4], encoder_src[cell_seq_length+4*b:cell_seq_length+4*b+2], encoder_src[cell_seq_length+4*(b+1):]],axis=0)\n",
    "            else:#source_length=60\n",
    "                encoder_src = tf.concat([encoder_src[:6 * a], encoder_src[6*a+3 : 6*a+6], encoder_src[6*a:6*a+3], encoder_src[6*(a+1):cell_seq_length+6*b], encoder_src[cell_seq_length+6*b+3:cell_seq_length+6*b+6], encoder_src[cell_seq_length+6*b:cell_seq_length+6*b+3], encoder_src[cell_seq_length+6*(b+1):]],axis=0)\n",
    "                \n",
    "            decoder_tgt = encoder_src\n",
    "                \n",
    "            return encoder_src, encoder_tgt, decoder_src, decoder_tgt\n",
    "        \n",
    "        dataset = dataset.map(preprocess)\n",
    "        if symmetry:\n",
    "            dataset = dataset.map(generate_symmetry)\n",
    "        dataset = dataset.repeat(num_epochs)\n",
    "        dataset = dataset.batch(batch_size)\n",
    "        dataset = dataset.prefetch(10)\n",
    "        iterator = dataset.make_one_shot_iterator()\n",
    "        encoder_input, encoder_target, decoder_input, decoder_target = iterator.get_next()\n",
    "        assert encoder_input.shape.ndims == 2\n",
    "        assert encoder_target.shape.ndims == 1\n",
    "        while encoder_target.shape.ndims < 2:\n",
    "            encoder_target = tf.expand_dims(encoder_target, axis=-1)\n",
    "        assert decoder_input.shape.ndims == 2\n",
    "        assert decoder_target.shape.ndims == 2\n",
    "        return encoder_input, encoder_target, decoder_input, decoder_target\n",
    "        \n",
    "        \n",
    "    else:\n",
    "        tf.logging.info(\"Data size : {}\".format(shape))\n",
    "        encoder_input = tf.convert_to_tensor(encoder_input, dtype=tf.int32)\n",
    "        encoder_input = tf.data.Dataset.from_tensor_slices(encoder_input)\n",
    "        def preprocess(encoder_src):\n",
    "            return encoder_src, tf.constant([SOS], dtype=tf.int32)\n",
    "        dataset = encoder_input.map(preprocess)\n",
    "        dataset = dataset.repeat(num_epochs)\n",
    "        dataset = dataset.batch(batch_size)\n",
    "        dataset = dataset.prefetch(10)\n",
    "        iterator = dataset.make_one_shot_iterator()\n",
    "        encoder_input, decoder_input = iterator.get_next()\n",
    "        assert encoder_input.shape.ndims == 2\n",
    "        return encoder_input, decoder_input\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "输入encoder input target加上decoder input target以及controller的参数，输出训练所需要的一些操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_ops(encoder_train_input, encoder_train_target, decoder_train_input, decoder_train_target, params, reuse=tf.AUTO_REUSE):\n",
    "    \n",
    "    with tf.variable_scope('EPD', reuse=reuse):\n",
    "        my_encoder = encoder.Model(\n",
    "            encoder_train_input,\n",
    "            encoder_train_target,\n",
    "            params,\n",
    "            tf.estimator.ModeKeys.TRAIN,\n",
    "            'Encoder',\n",
    "            reuse\n",
    "        )\n",
    "        encoder_outputs = my_encoder.encoder_outputs\n",
    "        encoder_state = my_encoder.arch_emb #shape = [batch_size, encoder_hidden_size]\n",
    "        encoder_state.set_shape([None, params['decoder_hidden_size']]) \n",
    "        encoder_state = tf.contrib.rnn.LSTMStateTuple(encoder_state, encoder_state)\n",
    "        encoder_state = (encoder_state,) * params['decoder_num_layers']\n",
    "        my_decoder = decoder.Model(encoder_outputs, encoder_state, decoder_train_input, decoder_train_target,params,tf.estimator.ModeKeys.TRAIN,'Decoder',reuse)\n",
    "        encoder_loss = my_encoder.loss\n",
    "        decoder_loss = my_decoder.loss\n",
    "        mse = encoder_loss\n",
    "        cross_entropy = decoder_loss\n",
    "        \n",
    "        total_loss = params['trade_off'] * encoder_loss + (1 - params['trade_off']) * decoder_loss + params['weight_decay'] * tf.add_n([tf.nn.l2_loss(v) for v in tf.trainable_variables()])\n",
    "        \n",
    "        tf.summary.scalar('training_loss', total_loss)\n",
    "        \n",
    "        global_step = tf.train.get_or_create_global_step()\n",
    "        learning_rate = tf.constant(params['lr'])\n",
    "        \n",
    "        if params['optimizer'] == 'sgd':\n",
    "            learning_rate = tf.cond(\n",
    "                global_step < params['start_decay_step'],\n",
    "                lambda: learning_rate,\n",
    "                lambda: tf.train.exponential_decay(\n",
    "                    learning_rate,\n",
    "                    (global_step - params['start_decay_step']),\n",
    "                    params['decay_steps'],\n",
    "                    params['decay_factor'],\n",
    "                    staircase=True),\n",
    "                name='calc_learning_rate')\n",
    "            \n",
    "            opt = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "        elif params['optimizer'] == 'adam':\n",
    "            assert float(params['lr']) <= 0.001, \"! High Adam learning rate %g\" % params['lr']\n",
    "            opt = tf.train.AdamOptimizer(learning_rate)\n",
    "        elif params['optimizer'] == 'adadelta':\n",
    "            opt = tf.train.AdadeltaOptimizer(learning_rate=learning_rate)\n",
    "        tf.summary.scalar(\"learning_rate\", learning_rate)\n",
    "        \n",
    "        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "        \n",
    "        with tf.control_dependencies(update_ops):\n",
    "            gradients, variables = zip(*opt.compute_gradients(total_loss))\n",
    "            grad_norm = tf.global_norm(gradients)\n",
    "            clipped_gradients,_ = tf.clip_by_global_norm(gradients, params['max_gradient_norm'])\n",
    "            \n",
    "            train_op = opt.apply_gradients(zip(clipped_gradients,variables), global_step=global_step)\n",
    "            \n",
    "        return mse, cross_entropy, total_loss, learning_rate, train_op, global_step, grad_norm\n",
    "    \n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这个函数输入处理完成的encoder-input和decoder-input以及控制组件的参数，返回的是predict-value，sample-id，new-sample-id\n",
    "\n",
    "训练模式，即 mode == tf.estimator.ModeKeys.TRAIN，必须提供的是 loss 和 train_op。\n",
    "验证模式，即 mode == tf.estimator.ModeKeys.EVAL，必须提供的是 loss。\n",
    "预测模式，即 mode == tf.estimator.ModeKeys.PREDICT，必须提供的是 predicitions。\n",
    "\n",
    "\n",
    "未完成：\n",
    "my_encoder.infer()\n",
    "\n",
    "my_decoder.decode()已实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predict_ops(encoder_predict_input, decoder_predict_input, params, reuse=tf.AUTO_REUSE):\n",
    "    encoder_predict_target = None\n",
    "    decoder_predict_target = None\n",
    "    with tf.variable_scope('EPD', reuse=reuse):\n",
    "        my_encoder = encoder.Model(encoder_predict_input, encoder_predict_target, params, tf.estimator.ModeKeys.PREDICT,'Encoder',reuse)\n",
    "        encoder_outputs = my_encoder.encoder_outputs\n",
    "        encoder_state = my_encoder.arch_emb\n",
    "        encoder_state.set_shape([None, params['decoder_hidden_size']])\n",
    "        encoder_state = tf.contrib.rnn.LSTMStateTuple(encoder_state, encoder_state)\n",
    "        encoder_state = (encoder_state, ) * params['decoder_num_layers']\n",
    "        my_decoder = decoder.Model(encoder_outputs, encoder_state, decoder_predict_input,decoder_predict_target, params, tf.estimator.ModeKeys.PREDICT, \"Decoder\",reuse)\n",
    "        \n",
    "        arch_emb, predict_value, new_arch_emb, new_arch_outputs = my_encoder.infer()\n",
    "        \n",
    "        sample_id = my_decoder.decode()\n",
    "        \n",
    "        encoder_state = new_arch_emb\n",
    "        encoder_state.set_shape([None, params['decoder_hidden_size']])\n",
    "        encoder_state = tf.contrib.rnn.LSTMStateTuple(encoder_state,encoder_state)\n",
    "        encoder_state = (encoder_state, )* params['decoder_num_layers']\n",
    "        tf.get_variable_scope().reuse_variables()\n",
    "        my_decoder = decoder.Model(new_arch_outputs, encoder_state,decoder_predict_input, decoder_predict_target,params, tf.estimator.ModeKeys.PREDICT,'Decoder')\n",
    "        new_sample_id = my_decoder.decode()\n",
    "        \n",
    "        return predict_value, sample_id, new_sample_id\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "输入训练参数，encoder_input, encoder_target,decoder_target来训练Encoder-Predictor-Decoder\n",
    "\n",
    "get_train_ops用来返回训练时所需要的一些操作，已经实现\n",
    "\n",
    "tf.summary_merge_all()是可视化相关的一个操作，以便tensorboard显示\n",
    "\n",
    "tf.summary.FileWriter 指定一个文件用来保存图\n",
    "\n",
    "train_writer.add_summary(train_summary,step)#调用train_writer的add_summary方法将训练过程以及训练步数保存 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(params, encoder_input, encoder_target, decoder_target):\n",
    "    with tf.Graph().as_default():\n",
    "        tf.logging.info('Training Encoder-Predictor-Decoder')\n",
    "        tf.logging.info('Preparing data')\n",
    "        shape = np.array(encoder_input).shape\n",
    "        N = shape[0]\n",
    "        encoder_train_input, encoder_train_target, decoder_train_input, decoder_train_target = input_fn(\n",
    "            encoder_input,\n",
    "            encoder_target,\n",
    "            decoder_target,\n",
    "            'train',\n",
    "            params['batch_size'],\n",
    "            None,\n",
    "            params['symmetry'],\n",
    "        )\n",
    "        \n",
    "        tf.logging.info('Building model')\n",
    "        train_mse, train_cross_entropy, train_loss, learning_rate, train_op, global_step, grad_norm = get_train_ops(\n",
    "            encoder_train_input,\n",
    "            encoder_train_target,\n",
    "            decoder_train_input,\n",
    "            decoder_train_target,\n",
    "            params\n",
    "        )\n",
    "        saver = tf.train.Saver(max_to_keep=10)\n",
    "        checkpoint_saver_hook = tf.train.CheckpointSaverHook(\n",
    "            params['model_dir'], \n",
    "            save_steps=params['batches_per_epoch']*params['save_frequency'],\n",
    "            saver=saver\n",
    "        )\n",
    "        hooks = [checkpoint_saver_hook]\n",
    "        merged_summary = tf.summary.merge_all()\n",
    "        tf.logging.info('Starting Session')\n",
    "        config = tf.ConfigProto(allow_soft_placement=True)\n",
    "        with tf.train.SingularMonitoredSession(config=config, hooks=hooks, checkpoint_dir=params['model_dir']) as sess:\n",
    "            writer = tf.summary.FileWriter(params['model_dir'], sess.graph)\n",
    "            start_time = time.time()\n",
    "            for step in range(int(params['train_epochs'] * params['batches_per_epoch'] )):\n",
    "                run_ops = [\n",
    "                    train_mse,\n",
    "                    train_cross_entropy,\n",
    "                    train_loss,\n",
    "                    learning_rate,\n",
    "                    train_op,\n",
    "                    global_step,\n",
    "                    grad_norm,\n",
    "                    merged_summary,\n",
    "                    \n",
    "                ]\n",
    "                \n",
    "                train_mse_v, train_cross_entropy_v, train_loss_v, learning_rate_v, _, global_step_v, gn_v, summary = sess.run(run_ops)\n",
    "                \n",
    "                writer.add_summary(summary, global_step_v)\n",
    "                \n",
    "                epoch = (global_step_v+1) // params['batches_per_epoch']\n",
    "                \n",
    "                curr_time = time.time()\n",
    "                \n",
    "                if (global_step_v+1)%100 == 0:\n",
    "                    log_string = \"epoch={:<6d} \".format(int(epoch))\n",
    "                    log_string += \"step={:<6d} \".format(int(global_step_v+1))\n",
    "                    log_string += \"se={:<6f} \".format(train_mse_v)\n",
    "                    log_string += \"cross_entropy={:<6f} \".format(train_cross_entropy_v)\n",
    "                    log_string += \"loss={:<6f} \".format(train_loss_v)\n",
    "                    log_string += \"learning_rate={:<8.4f} \".format(learning_rate_v)\n",
    "                    log_string += \"|gn|={:<8.4f} \".format(gn_v)\n",
    "                    log_string += \"mins={:<10.2f} \".format((curr_time - start_time) / 60)\n",
    "                    tf.logging.info(log_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "给定controller的一些参数，给定一个列表，列表里每个元素代表一个架构的code，输出的应该是各个架构对应的预测错误率\n",
    "\n",
    "有点不太明白这个函数的输出代表什么意思，因此暂时搁置该函数\n",
    "\n",
    "该函数的输出是新的架构，对每个输入的架构，进行更新，更新完成之后，得到的新的一系列架构new_sample_id_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(params, encoder_input):\n",
    "    with tf.Graph().as_default():\n",
    "        tf.logging.info(\"Generating new architectures using gradient desent with step size {}\".format(params['predict_lambda']))\n",
    "        tf.logging.info('Preparing data')\n",
    "        N = len(encoder_input)\n",
    "        encoder_input, decoder_input = input_fn(encoder_input, None, None, 'test', params['batch_size'],1,False)\n",
    "        predict_value, sample_id, new_sample_id = get_predict_ops(encoder_input, decoder_input, params)\n",
    "        tf.logging.info(\"Starting Session\")\n",
    "        config = tf.ConfigProto(allow_soft_placement=True)\n",
    "        new_sample_id_list=[]\n",
    "        with tf.train.SingularMonitoredSession(config=config, checkpoint_dir=params['model_dir']) as sess:\n",
    "            for _ in range(N // params['batch_size']):\n",
    "                new_sample_id_v = sess.run(new_sample_id)\n",
    "                new_sample_id_list.extend(new_sample_id_v.tolist())\n",
    "        return new_sample_id_list\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
