{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n<sos> 0\\n0     1\\n1     2\\n2     3\\n3     4\\n4     5\\n5     6\\nidentity 7\\nsep conv 8\\nmax pool 9\\navg pool 10\\n3x3      11\\n5x5      12\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "B = 5\n",
    "\n",
    "'''\n",
    "<sos> 0\n",
    "0     1\n",
    "1     2\n",
    "2     3\n",
    "3     4\n",
    "4     5\n",
    "5     6\n",
    "identity 7\n",
    "sep conv 8\n",
    "max pool 9\n",
    "avg pool 10\n",
    "3x3      11\n",
    "5x5      12\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "arch是字符串类型，代表了一个架构的描述，包含两个cell，先normal cell 后 reduction cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dag(arch):\n",
    "    if arch is None:\n",
    "        return None,None\n",
    "    #assume arch is the format [index,op ...] where index is in [0,5] and op in [0,10]\n",
    "    arch = list(map(int,arch.strip().split()))\n",
    "    length = len(arch)\n",
    "    conv_dag = arch[:length//2]\n",
    "    reduc_dag = arch[length//2:]\n",
    "    return conv_dag, reduc_dag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "根据点数和操作数，随机生成一个架构\n",
    "\n",
    "注意np.random.randint与random.randint不同，不包含上界"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_arch(n,num_nodes,num_ops = 7):\n",
    "    def _get_arch():\n",
    "        arch = []\n",
    "        for i in range(2,num_nodes+2):\n",
    "            p1 = np.random.randint(0,i)\n",
    "            op1 = np.random.randint(0,num_ops)\n",
    "            p2 = np.random.randint(0,i)\n",
    "            op2 = np.random.randint(0,num_ops)\n",
    "            arch.extend([p1,op1,p2,op2])\n",
    "        return arch\n",
    "    archs = [ [_get_arch(),_get_arch()] for i in range(n) ] #[[[conv],[reduc]]]\n",
    "    return archs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "给定一个参数的集合，计算参数个数\n",
    "\n",
    "np.prod表示计算后面一个list里元素的乘积"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_model_params(tf_variables):\n",
    "    num_vars = 0\n",
    "    for var in tf_variables:\n",
    "        num_vars += np.prod([dim.value for dim in var.get_shape()])\n",
    "    return num_vars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "根据输入，获取训练所需要的train_op, learning_rate, grad_norm, opt, grad_norms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_op(loss,\n",
    "                tf_variables,\n",
    "                train_step,\n",
    "                clip_mode=None,\n",
    "                grad_bound=None,\n",
    "                l2_reg=1e-4,\n",
    "                lr_warmup_val=None,\n",
    "                lr_warmup_steps=100,\n",
    "                lr_init=0.1,\n",
    "                lr_dec_start=0,\n",
    "                lr_dec_every=10000,\n",
    "                lr_dec_rate=0.1,\n",
    "                lr_dec_min=None,\n",
    "                lr_cosine=False,\n",
    "                lr_max=None,\n",
    "                lr_min=None,\n",
    "                lr_T_0=None,\n",
    "                lr_T_mul=None,\n",
    "                num_train_batches=None,\n",
    "                optim_algo=None,\n",
    "                sync_replicas=False,\n",
    "                num_aggregate=None,\n",
    "                num_replicas=None,\n",
    "                get_grad_norms=False,\n",
    "                moving_average=None):\n",
    "    '''\n",
    "    clip_mode:\"global\",\"norm\" or None\n",
    "    moving_average: store the moving average of parameters\n",
    "    '''\n",
    "    if l2_reg > 0:\n",
    "        l2_losses = []\n",
    "        for var in tf_variables:\n",
    "            l2_losses.append(tf.reduce_sum(var ** 2))\n",
    "        l2_loss = tf.add_n(l2_losses)\n",
    "        loss += l2_reg * l2_loss\n",
    "    \n",
    "    grads = tf.gradients(loss, tf_variables)\n",
    "    grad_norm = tf.global_norm(grads)\n",
    "    \n",
    "    grad_norms = {}\n",
    "    for v,g in zip(tf_variables, grads):\n",
    "        if v is None or g is None:\n",
    "            continue\n",
    "        if isinstance(g, tf.IndexedSlices):\n",
    "            grad_norms[v.name] = tf.sqrt(tf.reduce_sum(g.values ** 2))\n",
    "        else:\n",
    "            grad_norms[v.name] = tf.sqrt(tf.reduce_sum(g ** 2))\n",
    "        \n",
    "    if clip_mode is not None:\n",
    "        assert grad_bound is not None, \"Need grad_bound to clip gradients.\"\n",
    "        if clip_mode == \"global\":\n",
    "            grads, _ = tf.clip_by_global_norm(grads, grad_bound)\n",
    "        elif clip_mode == \"norm\":\n",
    "            clipped = []\n",
    "            for g in grads:\n",
    "                if isinstance(g, tf.IndexedSlices):\n",
    "                    c_g = tf.clip_by_norm(g.values, grad_bound)\n",
    "                    c_g = tf.IndexedSlices(c_g, g.indices)\n",
    "                else:\n",
    "                    c_g = tf.clip_by_norm(g, grad_bound)\n",
    "                clipped.append(g)\n",
    "            grads = clipped\n",
    "        else:\n",
    "            raise NotImplementedError(\"Unknown clip_mode {}\".format(clip_mode))\n",
    "    \n",
    "    if lr_cosine:\n",
    "        assert lr_max is not None, \"Need lr_max to use lr_cosine\"\n",
    "        assert lr_min is not None, \"Need lr_min to use lr_cosine\"\n",
    "        assert lr_T_0 is not None, \"Need lr_T_0 to use lr_cosine\"\n",
    "        assert lr_T_mul is not None, \"Need lr_T_mul to use lr_cosine\"\n",
    "        assert num_train_batches is not None, \"Need num_train_batches to use lr_cosine\"\n",
    "        \n",
    "        curr_epoch = tf.cast(train_step // num_train_batches, tf.int32)\n",
    "        \n",
    "        last_reset = tf.get_variable(\"last_reset\", initializer=0, dtype=tf.int32, trainable=False)\n",
    "        T_i = tf.get_variable(\"T_i\", initializer=lr_T_0, dtype=tf.int32, trainable=False)\n",
    "        T_curr = curr_epoch - last_reset\n",
    "        \n",
    "        def _update():\n",
    "            update_last_reset = tf.assign(last_reset, curr_epoch, use_locking = True)\n",
    "            update_T_i = tf.assign(T_i, T_i * lr_T_mul, use_locking=True)\n",
    "            with tf.control_dependencies([update_last_reset, update_T_i]):\n",
    "                rate = tf.to_float(T_curr) / tf.to_float(T_i) * 3.1415926\n",
    "                lr = lr_min + 0.5 * (lr_max - lr_min) * (1.0 + tf.cos(rate))\n",
    "            return lr\n",
    "        \n",
    "        def _no_update():\n",
    "            rate = tf.to_float(T_curr) / tf.to_float(T_i) * 3.1415926\n",
    "            lr = lr_min + 0.5 * (lr_max - lr_min) * (1.0 + tf.cos(rate))\n",
    "        \n",
    "        learning_rate = tf.cond(tf.greater_equal(T_curr, T_i), _update, _no_update)\n",
    "        \n",
    "    else:\n",
    "        learning_rate = tf.train.exponential_decay(lr_init, tf.maximum(train_step - lr_dec_start,0), lr_dec_every, lr_dec_rate, staircase=True)\n",
    "        \n",
    "        if lr_dec_min is not None:\n",
    "            learning_rate = tf.maximum(learning_rate, lr_dec_min)\n",
    "    \n",
    "    if lr_warmup_val is not None:\n",
    "        learning_rate = tf.cond(tf.less(train_step, lr_warmup_steps), lambda: lr_warmup_val, lambda: learning_rate)\n",
    "        \n",
    "    if optim_algo == 'momentum':\n",
    "        opt = tf.train.MomentumOptimizer(learning_rate, 0.9, use_locking=True, use_nesterov=True)\n",
    "    elif optim_algo == \"sgd\":\n",
    "        opt = tf.train.GradientDescentOptimizer(learning_rate, use_locking=True)\n",
    "    elif optim_algo == \"adam\":\n",
    "        opt = tf.train.AdamOptimizer(learning_rate, beta1=0.0, epsilon=1e-3, use_locking=True)\n",
    "    else:\n",
    "        raise ValueError(\"Unknown optim_algo {}\".format(optim_algo))\n",
    "    \n",
    "    if sync_replicas:\n",
    "        assert num_aggregate is not None, \"Need num_aggregate to sync\"\n",
    "        assert num_replicas is not None, \"Need num_replicas to sync\"\n",
    "        \n",
    "        opt = tf.train.SyncReplicasOptimizer(opt, replicas_to_aggregate=num_aggregate, total_num_replicas=num_replicas, use_locking=True)\n",
    "        \n",
    "    if moving_average is not None:\n",
    "        opt = tf.contrib.opt.MovingAverageOptimizer(opt, average_decay=moving_average)\n",
    "    \n",
    "    train_op = opt.apply_gradients( zip(grads, tf_variables), global_step = train_step )\n",
    "    \n",
    "    if get_grad_norms:\n",
    "        return train_op, learning_rate, grad_norm, opt, grad_norms\n",
    "    else:\n",
    "        return train_op, learning_rate, grad_norm, opt\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "给定格式化的cell，和branch length，输出对应的序列，在这里序列里所有的input的index都+1，也就是从0,1,2,3,4,5变为了1，2，3，4，5，6。当branchlength等于2的时候，所有的操作都+7，从0，1，2，3，4变为了7，8，9，10，11.但是当branch-length变为3的时候，却有点奇怪，类似于用两位编码的方式存起来了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_arch_to_seq(cell, branch_length):\n",
    "    assert branch_length in [2,3]\n",
    "    seq = []\n",
    "    def _parse_op(op):\n",
    "        if op == 0:\n",
    "            return 7, 12\n",
    "        if op == 1:\n",
    "            return 8, 11\n",
    "        if op == 2:\n",
    "            return 8, 12\n",
    "        if op == 3:\n",
    "            return 9, 11\n",
    "        if op == 4:\n",
    "            return 10, 11\n",
    "    \n",
    "    for i in range(B):\n",
    "        prev_node1 = cell[4 * i] + 1\n",
    "        prev_node2 = cell[4 * i + 2] + 1\n",
    "        if branch_length == 2:\n",
    "            op1 = cell[4*i+1] + 7\n",
    "            op2 = cell[4*i+3] + 7\n",
    "            seq.extend([prev_node1, op1, prev_node2, op2])\n",
    "        else:\n",
    "            op11, op12 = _parse_op(cell[4*i + 1])\n",
    "            op21, op22 = _parse_op(cell[4*i + 3])\n",
    "            seq.extend([prev_node1, op11, op12, prev_node2, op21, op22])\n",
    "    return seq\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "与上面的函数相反，给定seq和branch-length，解析出结构化的cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_seq_to_arch(seq, branch_length):\n",
    "    n = len(seq)\n",
    "    assert branch_length in [2,3]\n",
    "    assert n // 2 // 5 // 2 == branch_length\n",
    "    \n",
    "    def _parse_cell(cell_seq):\n",
    "        cell_arch = []\n",
    "        def _recover_op(op1,op2):\n",
    "            if op1 == 7:\n",
    "                return 0\n",
    "            if op1 == 8:\n",
    "                if op2 == 11:\n",
    "                    return 1\n",
    "                if op2 == 12:\n",
    "                    return 2\n",
    "            if op1 == 9:\n",
    "                return 3\n",
    "            if op1 == 10:\n",
    "                return 4\n",
    "        if branch_length == 2:\n",
    "            for i in range(B):\n",
    "                p1 = cell_seq[4*i] - 1\n",
    "                op1 = cell_seq[4*i+1] - 7\n",
    "                p2 = cell_seq[4*i+2] - 1\n",
    "                op2 = cell_seq[4*i+3] - 7\n",
    "                cell_arch.extend([p1,op1,p2,op2])\n",
    "            return cell_arch\n",
    "        else:\n",
    "            for i in range(B):\n",
    "                p1 = cell_seq[6*i] - 1\n",
    "                op1 = _recover_op(cell_seq[6*i+1],cell_seq[6*i+2])\n",
    "                p2 = cell_seq[6*i+3] - 1\n",
    "                op2 = _recover_op(cell_seq[6*i+4],cell_seq[6*i+5])\n",
    "                cell_arch.extend([p1,op1,p2,op2])\n",
    "            return cell_arch\n",
    "    conv_seq = seq[:n//2]\n",
    "    reduc_seq = seq[n//2:]\n",
    "    conv_arch = _parse_cell(conv_seq)\n",
    "    reduc_arch = _parse_cell(reduc_seq)\n",
    "    arch = [conv_arch, reduc_arch]\n",
    "    return arch\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
