{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import ops\n",
    "from tensorflow.contrib.seq2seq.python.ops.basic_decoder import BasicDecoderOutput\n",
    "\n",
    "INF = 1<<16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从初始化函数上来看和原本的堆叠没什么区别，可能在call函数上有一些区别"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionMultiCell(tf.nn.rnn_cell.MultiRNNCell):\n",
    "    '''A MultiCell with GNMT attention style'''\n",
    "    def __init__(self, attention_cell, cells, use_new_attention=True):\n",
    "        '''\n",
    "        Creates a GNMTAttentionMultiCell\n",
    "        \n",
    "        Args:\n",
    "            attention_cell: An instance of AttentionWrapper\n",
    "            cells: A list of RNNcell warpped with AttentionInputWrapper\n",
    "            use_new_attention: whether to use the attention generated from current step botton layer's output\n",
    "        '''\n",
    "        cells = [attention_cell] + cells\n",
    "        self.use_new_attention = use_new_attention\n",
    "        super(AttentionMultiCell, self).__init__(cells, state_is_tuple=True)\n",
    "    \n",
    "    def __call__(self, inputs, state, scope=None):\n",
    "        ''' Run the cell with bottom layer's attention copied to all upper layers '''\n",
    "        if not tf.contrib.framework.nest.is_sequence(state):\n",
    "            raise ValueError(\"Expected state to be a tuple of length %d, but received: %s\" %(len(self.state_size), state))\n",
    "        \n",
    "        with tf.variable_scope(scope or \"multi_rnn_cell\"):\n",
    "            new_states = []\n",
    "            \n",
    "            with tf.variable_scope(\"cell_0_attention\"):\n",
    "                attention_cell = self._cells[0]\n",
    "                attention_state = state[0]\n",
    "                cur_inp, new_attention_state = attention_cell(inputs, attention_state)\n",
    "                new_states.append(new_attention_state)\n",
    "            \n",
    "            for i in range(1, len(self._cells)):\n",
    "                with tf.variable_scope(\"cell_%d\" % i):\n",
    "                    cell = self._cells[i]\n",
    "                    cur_state = state[i]\n",
    "                    \n",
    "                    if not isinstance(cur_state, tf.contrib.rnn.LSTMStateTuple):\n",
    "                        raise TypeError(\" `state[{}]` must be a LSTMStateTuple \".format(i))\n",
    "                    #we always use new attention v2, where the attention output from the first layer is broadcast to all layers after that\n",
    "                    #It is empirically much better than using the attention input to the first layers with all subsequent layers.\n",
    "                    \n",
    "                    if self.use_new_attention:\n",
    "                        cur_state = cur_state._replace(h=tf.concat([cur_state.h, new_attention_state.attention],1))\n",
    "                    else:\n",
    "                        cur_state = cur_state._replace(h=tf.concat([cur_state.h, attention_state.attention], 1))\n",
    "                    \n",
    "                    cur_inp, new_state = cell(cur_inp, cur_state)\n",
    "                    new_states.append(new_state)\n",
    "                \n",
    "        return cur_inp, tuple(new_states)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MyDense类，输入self.vocab_size, branch_length = self.branch_length         \n",
    "\n",
    "只是大概知道call是怎么运行的，最后softmax那里，感觉两句话反了， 应该先执行mask，再进行softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDense(tf.layers.Dense):\n",
    "    def __init__(self,\n",
    "                 units,\n",
    "                 branch_length=3,\n",
    "                 activation=None,\n",
    "                 use_bias=True,\n",
    "                 kernel_initializer=None,\n",
    "                 bias_initializer=tf.zeros_initializer(),\n",
    "                 kernel_regularizer=None,\n",
    "                 bias_regularizer=None,\n",
    "                 activity_regularizer=None,\n",
    "                 kernel_constraint=None,\n",
    "                 bias_constraint=None,\n",
    "                 trainable=True,\n",
    "                 name=None,\n",
    "                 **kwargs      \n",
    "                ):\n",
    "        super(MyDense, self).__init__(\n",
    "            units,\n",
    "            activation=None,\n",
    "            use_bias=True,\n",
    "            kernel_initializer=None,\n",
    "            bias_initializer=tf.zeros_initializer()\n",
    "            kernel_regularizer=None,\n",
    "            bias_regularizer=None,\n",
    "            activity_regularizer=None,\n",
    "            kernel_constraint=None,\n",
    "            bias_constraint=None,\n",
    "            trainable=True,\n",
    "            name=None,\n",
    "            **kwargs)\n",
    "        self.branch_length = branch_length\n",
    "    \n",
    "    def call(self, inputs, time=None):\n",
    "        inputs = tf.convert_to_tensor(inputs, dtype=self.dtype)\n",
    "        shape = inputs.get_shape().as_list()\n",
    "        if len(shape) >2 : #not_predicting\n",
    "            #broadcasting is required for the inputs\n",
    "            outputs = tf.tensordot(inputs, self.kernel, [[len(shape) - 1], [0]])\n",
    "            #reshape the output back to the original ndim of the input\n",
    "            output_shape = shape[:-1] + [self.units]\n",
    "            outputs.set_shape(output_shape)\n",
    "        else:\n",
    "            #predicting\n",
    "            outputs = tf.matmul(inputs, self.kernel)\n",
    "        if self.use_bias:\n",
    "            outputs = tf.nn.bias_add(outputs, self.bias)\n",
    "        if self.activation is not None:\n",
    "            outputs = self.activation(outputs)\n",
    "        \n",
    "        def _index(x):\n",
    "            index = tf.cast(time / self.branch_length % 10 / 2, dtype=tf.int32) + 3\n",
    "            assert x.shape.ndims == 2\n",
    "            mask_1 = tf.zeros_like(x)[:, :1]\n",
    "            ones = tf.ones_like(x)[:, 1:index]\n",
    "            mask_2 = tf.zeros_like(x)[:, index:]\n",
    "            mask = tf.concat([mask_1, ones, mask_2], axis=-1)\n",
    "            return mask * x\n",
    "        def _op(x):\n",
    "            mask_1 = tf.zeros_like(x)[:, :7]\n",
    "            ones = tf.ones_like(x)[:, 7:]\n",
    "            mask = tf.concat([mask_1, ones], axis=-1)\n",
    "            return mask * x\n",
    "        if time is not None: #predicting\n",
    "            outputs = tf.nn.softmax(outputs)\n",
    "            outputs = tf.cond(tf.equal(tf.mod(time, self.branch_length), tf.constant(0)), lambda : _index(outputs), lambda : _op(outputs))\n",
    "        return outputs\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MyDecoder(cell, helper, decoder_initial_state)\n",
    "\n",
    "由于MyDecoder需要被dynamic_decode进行调用，使用其中的init函数以及step函数，因此这里补充了step函数，step函数输入的是time,inputs,state输出的是outputs,next_state, next_inputs, finished\n",
    "\n",
    "def step(self, time, inputs, state, name=None):\n",
    "       with ops.name_scope(name, \"BasicDecoderStep\", (time, inputs, state)):\n",
    "         cell_outputs, cell_state = self._cell(inputs, state)\n",
    "         if self._output_layer is not None:\n",
    "           #如果设置了output层，将cell的输出进行映射\n",
    "           cell_outputs = self._output_layer(cell_outputs)\n",
    "         #根据输出结果，选出想要的答案，比如说贪婪法选择概率最大的单词，Scheduled使用某种概率分布进行采样等等\n",
    "         sample_ids = self._helper.sample(time=time, outputs=cell_outputs, state=cell_state)\n",
    "         #得到输出结果将其转化为下一时刻输入。train的时候就是decoder_inputs的下一时刻，预测的时候将选出的单词进行embedding即可\n",
    "         (finished, next_inputs, next_state) = self._helper.next_inputs(time=time, outputs=cell_outputs, state=cell_state, sample_ids=sample_ids)\n",
    "       outputs = BasicDecoderOutput(cell_outputs, sample_ids)#nameTulpe，将其一起作为outputs变量\n",
    "       return (outputs, next_state, next_inputs, finished)\n",
    "       \n",
    "\n",
    "其实Decoder这一块还是很迷糊， 这个MyDecoder类看起来和原本的BasicDecoder似乎没有什么区别"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDecoder(tf.contrib.seq2seq.BasicDecoder):\n",
    "    def __init__(self, cell, helper, initial_state, output_layer=None):\n",
    "        super(MyDecoder, self).__init__(cell, helper, initial_state, output_layer)\n",
    "    \n",
    "    def step(self, time, inputs, state, name=None):\n",
    "        with ops.name_scope(name, 'BasicDecoderStep', (time,inputs, state)):\n",
    "            cell_outputs, cell_state = self._cell(inputs, state)\n",
    "            \n",
    "            if self._output_layers is not None:\n",
    "                cell_outputs = self._output_layer(cell_outputs,time)\n",
    "            sample_ids = self._helper.sample(time=time, outputs=cell_outputs, state=cell_state)\n",
    "            (finished, next_inputs, next_state) = self._helper.next_inputs(time=time, outputs=cell_outputs, state=cell_state, sample_ids=sample_ids)\n",
    "            \n",
    "        outputs = BasicDecoderOutput(cell_outputs, sample_ids)\n",
    "        return (outputs, next_state, next_inputs, finished)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里decoder是一个sequence-to-sequence的模型\n",
    "\n",
    "Decoder类Decoder(self.params, self.mode, self.W_emb, self.output_layer)\n",
    "\n",
    "decoder.build_decoder(self.encoder_outputs, self.encoder_state, self.target_input, self.batch_size)\n",
    "\n",
    "self.build_decoder_cell(encoder_outputs, encoder_state)\n",
    "\n",
    "build_decoder_cell中的self.attn并不是很明白是什么意思，其对应着变量memory的赋值，不知该变量有何用\n",
    "\n",
    "tf.contrib.seq2seq.tile_batch:\n",
    "\n",
    "For each tensor t in a (possibly nested structure) of tensors, this function takes a tensor t shaped [batch_size, s0, s1, ...] composed of minibatch entries t[0], ..., t[batch_size - 1] and tiles it to have a shape [batch_size * multiplier, s0, s1, ...] composed of minibatch entries t[0], t[0], ..., t[1], t[1], ... where each minibatch entry is repeated multiplier times\n",
    "\n",
    "beam_width用来成倍增长batch_size\n",
    "\n",
    "alignment_history其实有点没太明白，只是大致知道是在可视化方面的一个东西，在预测模式下，才有可能变成true\n",
    "\n",
    "AttentionMultiCell尚未实现，大概知道是凑成多层LSTM并且首层采用注意力机制的结构，起着和tf.contrib.rnn.MultiRNNCell函数一样的效果\n",
    "\n",
    "self.pass_hidden_state不明所以，暂时不管\n",
    "\n",
    "helper的理解有点迷糊\n",
    "\n",
    "现在才知道，为何controller里用的序列，必须都是大于1的，因为0是是作为终止符起始符存在的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder():\n",
    "    def __init__(self, params, mode, embedding_decoder, output_layer):\n",
    "        self.num_layers = params['decoder_num_layers']\n",
    "        self.hidden_size = params['decoder_hidden_size']\n",
    "        self.length = params['decoder_length']\n",
    "        self.source_length = params['encoder_length'] #encoder_outputs.shape = [batch_size, encoder_length, ..]\n",
    "        self.vocab_size = params['decoder_vocab_size']\n",
    "        self.dropout = params['decoder_dropout']\n",
    "        self.embedding_decoder = embedding_decoder\n",
    "        self.output_layer = output_layer\n",
    "        self.time_major = params['time_major']\n",
    "        self.beam_width = params['predict_beam_width']\n",
    "        self.attn = params['attention']\n",
    "        self.pass_hidden_state = params.get('pass_hidden_state',True)\n",
    "        self.mode = mode\n",
    "    \n",
    "    def build_decoder(self, encoder_outputs, encoder_state, target_input, batch_size):\n",
    "        tgt_sos_id = tf.constant(0)\n",
    "        tgt_eos_id = tf.constant(0)\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        with tf.variable_scope('decoder') as decoder_scope:\n",
    "            cell, decoder_initial_state = self.build_decoder_cell(encoder_outputs, encoder_state)\n",
    "            if self.mode != tf.estimator.ModeKeys.PREDICT:\n",
    "                if self.time_major:\n",
    "                    target_input = tf.transpose(target_input)\n",
    "                decoder_emb_inp = tf.nn.embedding_lookup(self.embedding_decoder, target_input)\n",
    "                #helper\n",
    "                helper = tf.contrib.seq2seq.TrainingHelper(decoder_emb_inp, tf.tile([self.length],[self.batch_size]), time_major=self.time_major)\n",
    "                #Decoder\n",
    "                my_decoder = MyDecoder(cell, helper, decoder_initial_state)\n",
    "                \n",
    "                #Dynamic decoding\n",
    "                outputs, final_context_state, _ = tf.contrib.seq2seq.dynamic_decode(my_decoder, output_time_major=self.time_major,swap_memory=False,scope=decoder_scope)\n",
    "                \n",
    "                sample_id = outputs.sample_id\n",
    "                \n",
    "                logits = self.output_layer(outputs.run_output)\n",
    "            else:\n",
    "                beam_width = self.beam_width\n",
    "                start_tokens = tf.fill([self.batch_size],tgt_sos_id)\n",
    "                end_token = tat_eos_id\n",
    "                if beam_width > 0:\n",
    "                    my_decoder = tf.contrib.seq2seq.BeamSearchDecoder(cell=cell, \n",
    "                                                                      embedding=self.embedding_decoder,\n",
    "                                                                      start_tokens=start_tokens,\n",
    "                                                                      end_token=end_token,\n",
    "                                                                      initial_state=decoder_initial_state,\n",
    "                                                                      beam_width=beam_width,\n",
    "                                                                      output_layer=self.output_layer)\n",
    "                else:\n",
    "                    helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(self.embedding_decoder,start_tokens, end_token)\n",
    "                    \n",
    "                    my_decoder = Mydecoder(cell, helper, decoder_initial_state, output_layer=self.output_layer)\n",
    "                \n",
    "                outputs, final_context_state, _ = tf.contrib.seq2seq.dynamic_decode(my_decoder, maximum_iterations=self.length, output_time_major=self.time_major,swap_memory=True, scope=decoder_scope)\n",
    "                \n",
    "                if beam_width >0:\n",
    "                    logits = tf.no_op()\n",
    "                    sample_id = outputs.predicted_ids\n",
    "                else:\n",
    "                    logits = outputs.rnn_output\n",
    "                    sample_id = outputs.sample_id\n",
    "                \n",
    "        return logits, sample_id, final_context_state\n",
    "                    \n",
    "                \n",
    "    \n",
    "    def build_decoder_cell(self, encoder_outputs, encoder_state):\n",
    "        source_sequence_length = tf.tile([self.source_length],[self.batch_size]) #[length,length,...,length]\n",
    "        \n",
    "        if self.attn:\n",
    "            if self.time_major:\n",
    "                memory = tf.transpose(encoder_outputs, [1,0,2])\n",
    "            else:\n",
    "                memory = encoder_outputs\n",
    "        \n",
    "        if self.mode == tf.estimator.ModeKeys.PREDICT and self.beam_width > 0:\n",
    "            if self.attn:\n",
    "                memory = tf.contrib.seq2seq.tile_batch(memory, multiplier=self.beam_width)\n",
    "                \n",
    "            source_sequence_length = tf.contrib.seq2seq.tile_batch(source_sequence_length, multiplier=self.beam_width)\n",
    "            \n",
    "            encoder_state = tf.contrib.seq2seq.tile_batch(encoder_state, multiplier=self.beam_width)\n",
    "            \n",
    "            batch_size = self.batch_size * self.beam_width\n",
    "        else:\n",
    "            batch_size = self.batch_size\n",
    "        \n",
    "        if self.attn:\n",
    "            attention_mechanism = self.create_attention_mechanism('normed_bahdanau', self.hidden_size, memory, source_sequence_length)\n",
    "        \n",
    "        cell_list = []\n",
    "        for i in range(self.num_layers):\n",
    "            lstm_cell = tf.contrib.rnn.LSTMCell(self.hidden_size, initializer=tf.orthogonal_initializer())\n",
    "            lstm_cell = tf.contrib.rnn.DropoutWrapper(lstm_cell, output_keep_prob=1- self.dropout)\n",
    "            cell_list.append(lstm_cell)\n",
    "        if self.attn:\n",
    "            attention_cell = cell_list.pop(0)\n",
    "            alignment_history = (self.mode == tf.estimator.ModeKeys.PREDICT and self.beam_width == 0)\n",
    "            attention_cell = tf.contrib.seq2seq.AttentionWrapper(\n",
    "                attention_cell,\n",
    "                attention_mechanism,\n",
    "                attention_layer_size=None,\n",
    "                output_attention=False,\n",
    "                alignment_history=alignment_history,\n",
    "                name='attention')\n",
    "            cell = AttentionMultiCell(attention_cell, cell_list)\n",
    "        else:\n",
    "            if len(cell_list) == 1:\n",
    "                cell = cell_list[0]\n",
    "            else:\n",
    "                cell = tf.contrib.rnn.MultiRNNCell(cell_list)\n",
    "        \n",
    "        if self.pass_hidden_state:\n",
    "            decoder_initial_state = tuple(\n",
    "                zs.clone(cell_state=es) if isinstance(zs, tf.contrib.seq2seq.AttentionWrapperState) else es for zs,es in zip(cell.zero_state(batch_size,tf.float32),encoder_state))\n",
    "        else:\n",
    "            decoder_initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "        \n",
    "        return cell, decoder_initial_state\n",
    "                \n",
    "        \n",
    "            \n",
    "    \n",
    "    def create_attention_mechanism(self, attention_option, num_units, memory, source_sequence_length):\n",
    "        if attention_option == 'luong':\n",
    "            attention_mechanism = tf.contrib.seq2seq.LuongAttention(num_units,memory, memory_sequence_length=source_sequence_length)\n",
    "        elif attention_option == \"scaled_luong\":\n",
    "            attention_mechanism = tf.contrib.seq2seq.LuongAttention(num_units,memory, memory_sequence_length=source_sequence_length,scale=True)\n",
    "        elif attention_option == 'bahdanau':\n",
    "            attention_mechanism = tf.contrib.seq2seq.BahdanauAttention(num_units,memory, memory_sequence_length=source_sequence_length)\n",
    "        elif attention_option == 'normed_bahdanau':\n",
    "            attention_mechanism = tf.contrib.seq2seq.BahdanauAttention(num_units,memory, memory_sequence_length=source_sequence_length,normalize=True)\n",
    "        \n",
    "        else:\n",
    "            raise ValueError('Unknown attention option %s' % attention_option)\n",
    "        \n",
    "        return attention_mechanism\n",
    "        \n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model类，初始化需要输入encoder_outputs, encoder_state,不明所以的target_input, target和controller的参数params，以及模式\n",
    "\n",
    "self.output_layer是一个MyDense类，用来代表一个层的操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Model(object):\n",
    "    def __init__(\n",
    "        self,\n",
    "        encoder_outputs,\n",
    "        encoder_state,\n",
    "        target_input,\n",
    "        target,\n",
    "        params,\n",
    "        mode,\n",
    "        scope = None,\n",
    "        reuse = tf.AUTO_REUSE\n",
    "    ):\n",
    "        \n",
    "        self.params = params\n",
    "        self.encoder_outputs = encoder_outputs\n",
    "        self.encoder_state = encoder_state\n",
    "        self.target_input = target_input\n",
    "        self.target = target\n",
    "        self.batch_size = tf.shape(self.target_input)[0]\n",
    "        self.mode = mode\n",
    "        self.vocab_size = params['decoder_vocab_size']\n",
    "        self.num_layers = params['decoder_num_layers']\n",
    "        self.decoder_length = params['decoder_length']\n",
    "        self.time_major = params['time_major']\n",
    "        self.hidden_size = params['decoder_hidden_size']\n",
    "        self.weight_decay = params['weight_decay']\n",
    "        self.is_training = mode == tf.estimator.ModeKeys.TRAIN\n",
    "        if not self.is_training:\n",
    "            self.params['decoder_dropout'] = 0.0\n",
    "        self.branch_length = self.decoder_length // 2 // 5 // 2 #2 types of cells, 5 nodes, 2 branches\n",
    "        \n",
    "        initializer = tf.random_uniform_initializer(-0.1,0.1)\n",
    "        tf.get_variable_scope().set_initializer(initializer)\n",
    "        \n",
    "        self.build_graph(scope=scope, reuse=reuse)\n",
    "        \n",
    "    def build_graph(self, scope=None, reuse=tf.AUTO_REUSE):\n",
    "        tf.logging.info(\"# creating %s graph\"% self.mode )\n",
    "        #Decoder\n",
    "        \n",
    "        with tf.variable_scope(scope, reuse=reuse):\n",
    "            #Embeddings\n",
    "            self.W_emb = tf.get_variable('W_emb', [self.vocab_size, self.hidden_size])\n",
    "            #Projection\n",
    "            with tf.variable_scope(\"decoder/output_projection\"):\n",
    "                self.output_layer = MyDense(\n",
    "                    self.vocab_size,\n",
    "                    branch_length = self.branch_length,\n",
    "                    use_bias=False,\n",
    "                    name = \"output_projection\"\n",
    "                )\n",
    "            self.logits, self.sample_id, self.final_context_state = self.build_decoder()\n",
    "            \n",
    "            ##loss\n",
    "            if self.mode != tf.estimator.ModeKeys.PREDICT:\n",
    "                self.compute_loss()\n",
    "            else:\n",
    "                self.loss = None\n",
    "                self.total_loss = None\n",
    "        \n",
    "    def build_decoder(self):\n",
    "        decoder = Decoder(self.params, self.mode, self.W_emb, self.output_layer)\n",
    "        logits, sample_id, final_context_state = decoder.build_decoder(self.encoder_outputs, self.encoder_state, self.target_input, self.batch_size)\n",
    "        \n",
    "        return logits, sample_id, final_context_state\n",
    "    \n",
    "    def get_max_time(self, tensor):\n",
    "        time_axis=0 if self.time_major else 1\n",
    "        return tensor.shape[time_axis].value or tf.shape(tensor)[time_axis]\n",
    "    \n",
    "    def compute_loss(self):\n",
    "        \"\"\"compute optimization loss\"\"\"\n",
    "        \n",
    "        target_output = self.target\n",
    "        if self.time_major:\n",
    "            target_output = tf.transpose(target_output)\n",
    "        \n",
    "        max_time = self.get_max_time(target_output)\n",
    "        crossent = tf.losses.sparse_softmax_cross_entropy(labels=target_output, logits=self.logits)\n",
    "        \n",
    "        tf.identity(crossent, 'cross_entropy')\n",
    "        self.loss = crossent\n",
    "        total_loss = crossent + self.weight_decay * tf.add_n([tf.nn.l2_loss(v) for v in tf.trainable_variables()])\n",
    "        self.total_loss = total_loss\n",
    "        \n",
    "    \n",
    "    def infer(self):\n",
    "        assert self.mode == tf.estimator.ModeKeys.PREDICT\n",
    "        return {\n",
    "            'logits': self.logits\n",
    "            'sample_id': self.sample_id\n",
    "        }\n",
    "    \n",
    "    def decode(self):\n",
    "        res = self.infer()\n",
    "        sample_id = res['sample_id']\n",
    "        #make sure outputs is of shape [batch_size, time, 1]\n",
    "        if self.time_major:\n",
    "            try:\n",
    "                sample_id = tf.transpose(sample_id, [1,0])\n",
    "            except:\n",
    "                sample_id = tf.transpose(sample_id, [1,0,2])\n",
    "        return sample_id\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "总结来说，encoder_outputs只会在注意力机制那里用到，其他用不到，encoder_state是用来给decoder里的cell进行初始化的操作，也就是说这两个是在build_cell里使用到，其余均没有再次使用，\n",
    "\n",
    "target_inputs是真正的decoder的时候，训练时所用的输入，存在helper里面"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
