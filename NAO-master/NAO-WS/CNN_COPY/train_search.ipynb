{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第一步，先引入一些模块，后面可能会有用，暂时未知"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import json\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "import utils\n",
    "from calculate_params import calculate_params\n",
    "from model_search import valid as child_valid\n",
    "import controller\n",
    "\n",
    "\n",
    "\n",
    "import subprocess\n",
    "import numpy as np\n",
    "\n",
    "import copy\n",
    "\n",
    "import math\n",
    "import time\n",
    "from model_search import train as child_train\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "导入一些参数,\n",
    "\n",
    "cutout:\n",
    "\n",
    "To implement cutout, we simply apply a fixed-size zeromask\n",
    "to a random location of each input image during each\n",
    "epoch of training\n",
    "指的是一种对图像处理的方法，可以看做data augmentation的手段\n",
    "\n",
    "action = 'store_true'，表示如果不加 --命令名称，对应的值是false，加上，对应的值就是true，无需赋值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreAction(option_strings=['--controller_predict_lambda'], dest='controller_predict_lambda', nargs=None, const=None, default=1, type=<type 'float'>, choices=None, help=None, metavar=None)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser.add_argument('--mode',type=str,default='train',choices=['train','test'])\n",
    "parser.add_argument('--data_path',type=str,default='/tmp/cifar10_data')\n",
    "parser.add_argument('--eval_dataset',type=str,default='valid',choices=['valid','test','both'])\n",
    "parser.add_argument('--output_dir',type=str,default='models')\n",
    "\n",
    "\n",
    "parser.add_argument('--child_sample_policy',type=str,default=None)\n",
    "parser.add_argument('--child_batch_size',type=int,default=128)\n",
    "parser.add_argument('--child_eval_batch_size',type=int,default=128)\n",
    "parser.add_argument('--child_num_epochs',type=int,default=150)\n",
    "parser.add_argument('--child_lr_dec_every',type=int,default=100)\n",
    "parser.add_argument('--child_num_layers',type=int,default=5)\n",
    "parser.add_argument('--child_num_cells',type=int,default=5)\n",
    "parser.add_argument('--child_out_filters',type=int,default=20)\n",
    "parser.add_argument('--child_out_filters_scale',type=int,default=1)\n",
    "parser.add_argument('--child_num_branches',type=int,default=5)\n",
    "parser.add_argument('--child_num_aggregate',type=int,default=None)\n",
    "parser.add_argument('--child_num_replicas',type=int,default=None)\n",
    "parser.add_argument('--child_lr_T_0',type=int,default=None)\n",
    "parser.add_argument('--child_lr_T_mul',type=int,default=None)\n",
    "parser.add_argument('--child_cutout_size',type=int,default=None)\n",
    "parser.add_argument('--child_grad_bound',type=float,default=5.0)\n",
    "parser.add_argument('--child_lr',type=float,default=0.1)\n",
    "parser.add_argument('--child_lr_dec_rate',type=float,default=0.1)\n",
    "parser.add_argument('--child_lr_max',type=float,default=None)\n",
    "parser.add_argument('--child_lr_min',type=float,default=None)\n",
    "parser.add_argument('--child_keep_prob',type=float,default=0.5)\n",
    "parser.add_argument('--child_drop_path_keep_prob',type=float,default=1.0)\n",
    "parser.add_argument('--child_l2_reg',type=float,default=1e-4)\n",
    "parser.add_argument('--child_fixed_arc',type=str,default=None)\n",
    "parser.add_argument('--child_use_aus_heads',action='store_true',default=False)\n",
    "parser.add_argument('--child_sync_replicas',action='store_true',default=False)\n",
    "parser.add_argument('--child_lr_cosine',action='store_true',default=False)\n",
    "parser.add_argument('--child_eval_every_epochs',type=str,default='30')\n",
    "parser.add_argument('--child_arch_pool',type=str,default=None)\n",
    "parser.add_argument('--child_data_format',type=str,default='NHWC',choices=['NHWC','NCHW'])\n",
    "\n",
    "#encoder\n",
    "parser.add_argument('--controller_num_seed_arch',type=int,default=1000)\n",
    "parser.add_argument('--controller_encoder_num_layers',type=int,default=1)\n",
    "parser.add_argument('--controller_encoder_hidden_size',type=int,default=96)\n",
    "parser.add_argument('--controller_encoder_emb_size',type=int,default=32)\n",
    "\n",
    "#predictor\n",
    "parser.add_argument('--controller_mlp_num_layers',type=int,default=0)\n",
    "parser.add_argument('--controller_mlp_hidden_size',type=int,default=200)\n",
    "\n",
    "#decoder\n",
    "parser.add_argument('--controller_decoder_num_layers',type=int,default=1)\n",
    "parser.add_argument('--controller_decoder_hidden_size',type=int,default=96)\n",
    "\n",
    "parser.add_argument('--controller_source_length',type=int,default=60)\n",
    "parser.add_argument('--controller_encoder_length',type=int,default=20)\n",
    "parser.add_argument('--controller_decoder_length',type=int, default=60)\n",
    "\n",
    "parser.add_argument('--controller_encoder_dropout',type=float,default=0.1)\n",
    "parser.add_argument('--controller_mlp_dropout',type=float,default=0.1)\n",
    "parser.add_argument('--controller_decoder_dropout',type=float,default=0.0)\n",
    "\n",
    "parser.add_argument('--controller_weight_decay',type=float,default=1e-4)\n",
    "\n",
    "parser.add_argument('--controller_encoder_vocab_size',type=int,default=12)\n",
    "parser.add_argument('--controller_decoder_vocab_size',type=int,default=12)\n",
    "\n",
    "parser.add_argument('--controller_trade_off',type=float,default=0.8)\n",
    "parser.add_argument('--controller_train_epochs',type=int,default=300)\n",
    "parser.add_argument('--controller_save_frequency',type=int,default=10)\n",
    "parser.add_argument('--controller_batch_size',type=int,default=100)\n",
    "parser.add_argument('--controller_lr',type=float,default=0.001)\n",
    "parser.add_argument('--controller_optimizer',type=str,default='adam')\n",
    "parser.add_argument('--controller_start_decay_step',type=int,default=100)\n",
    "parser.add_argument('--controller_decay_steps',type=int,default=1000)\n",
    "parser.add_argument('--controller_decay_factor',type=float,default=0.9)\n",
    "parser.add_argument('--controller_attention',action='store_true',default=False)\n",
    "parser.add_argument('--controller_max_gradient_norm',type=float,default=5.0)\n",
    "parser.add_argument('--controller_time_major',action='store_true',default=False)\n",
    "parser.add_argument('--controller_symmetry',action='store_true',default=False)\n",
    "parser.add_argument('--controller_predict_beam_width',type=int,default=0)\n",
    "parser.add_argument('--controller_predict_lambda',type=float,default=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以上的这些参数会被保存到FLAGS这个字典中，下面是从FLAGS中获取child模型参数的函数，函数返回的依然是一个字典，只不过保存的value值仅限于与child有关。其中‘arch_pool’这一键值对应的是一个list，这个list里，每个元素，都是一个架构（包含normal cell和reduction cell）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_child_model_params():\n",
    "    params = {\n",
    "        'data_dir': FLAGS.data_path,\n",
    "        'model_dir': os.path.join(FLAGS.output_dir,'child'),\n",
    "        'sample_policy': FLAGS.child_sample_policy,\n",
    "        'batch_size': FLAGS.child_batch_size,\n",
    "        'eval_batch_size': FLAGS.child_eval_batch_size,\n",
    "        'num_epochs': FLAGS.child_num_epochs,\n",
    "        'lr_dec_every': FLAGS.child_lr_dec_every,\n",
    "        'num_layers': FLAGS.child_num_layers,\n",
    "        'num_cells': FLAGS.child_num_cells,\n",
    "        'out_filters': FLAGS.child_out_filters,\n",
    "        'out_filters_scale': FLAGS.child_out_filters_scale,\n",
    "        'num_aggregate': FLAGS.child_num_aggregate,\n",
    "        'num_replicas': FLAGS.child_num_replicas,\n",
    "        'lr_T_0': FLAGS.child_lr_T_0,\n",
    "        'lr_T_mul': FLAGS.child_lr_t_mul,\n",
    "        'cutout_size': FLAGS.child_cutout_size,\n",
    "        'grad_bound': FLAGS.child_grad_bound,\n",
    "        'lr_dec_rate': FLAGS.child_lr_dec_rate,\n",
    "        'lr_max': FLAGS.child_lr_max,\n",
    "        'lr_min': FLAGS.child_lr_min,\n",
    "        'drop_path_keep_prob': FLAGS.child_drop_path_keep_prob,\n",
    "        'keep_prob': FLAGS.child_keep_prob,\n",
    "        'l2_reg': FLAGS.child_l2_reg,\n",
    "        'fixed_arc': FLAGS.child_fixed_arc,\n",
    "        'use_aux_heads': FLAGS.child_use_aux_heads,\n",
    "        'sync_replicas': FLAGS.child_sync_replicas,\n",
    "        'lr_cosine': FLAGS.child_lr_cosine,\n",
    "        'eval_every_epochs': eval(FLAGS.child_eval_every_epochs),\n",
    "        'data_format': FLAGS.child_data_format,\n",
    "        'lr': FLAGS.child_lr,\n",
    "        'arch_pool': None,       \n",
    "    }\n",
    "    if FLAGS.child_arch_pool is not None:\n",
    "        with open(FLAGS.child_arch_pool) as f:\n",
    "            archs = f.read().splitlines()\n",
    "            archs = list(map(utils.build_dag,archs))\n",
    "            params['arch_pool'] = archs\n",
    "    if os.path.exists(os.path.join(params['model_dir'],'arch_pool')):\n",
    "        tf.logging.info('Found arch_pool in child model dir, loading')\n",
    "        with open(os.path.join(params['model_dir'],'arch_pool')) as f:\n",
    "            archs = f.read().splitlines()\n",
    "            archs = list(map(utils.build_dag,archs))\n",
    "            params['arch_pool'] = archs\n",
    "    return params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从FLAGS中提出与controller模型有关的参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_controller_params():\n",
    "    params = {\n",
    "        'model_dir': os.path.join(FLAGS.output_dir,'controller'),\n",
    "        'num_seed_arch': FLAGS.controller_num_seed_arch,\n",
    "        'encoder_num_layers': FLAGS.controller_encoder_num_layers,\n",
    "        'encoder_hidden_size': FLAGS.controller_encoder_hidden_size,\n",
    "        'encoder_emb_size': FLAGS.controller_encoder_emb_size,\n",
    "        'mlp_num_layers': FLAGS.controller_mlp_num_layers,\n",
    "        'mlp_hidden_size': FLAGS.controller_mlp_hidden_size,\n",
    "        'decoder_num_layers': FLAGS.controller_decoder_num_layers,\n",
    "        'decoder_hidden_size': FLAGS.controller_decoder_hidden_size,\n",
    "        'source_length': FLAGS.controller_source_length,\n",
    "        'encoder_length': FLAGS.controller_encoder_length,\n",
    "        'decoder_length': FLAGS.controller_decoder_length,\n",
    "        'encoder_dropout': FLAGS.controller_encoder_dropout,\n",
    "        'mlp_dropout': FLAGS.controller_mlp_dropout,\n",
    "        'decoder_dropout': FLAGS.controller_decoder_dropout,\n",
    "        'weight_decay': FLAGS.controller_weight_decay,\n",
    "        'encoder_vocab_size': FLAGS.controller_encoder_vocab_size,\n",
    "        'decoder_vocab_size': FLAGS.controller_decoder_vocab_size,\n",
    "        'trade_off': FLAGS.controller_trade_off,\n",
    "        'train_epochs': FLAGS.controller_train_epochs,\n",
    "        'save_frequency': FLAGS.controller_save_frequency,\n",
    "        'batch_size': FLAGS.controller_batch_size,\n",
    "        'lr': FLAGS.controller_lr,\n",
    "        'optimizer': FLAGS.controller_optimizer,\n",
    "        'start_decay_step': FLAGS.controller_start_decay_step,\n",
    "        'decay_steps': FLAGS.controller_decay_steps,\n",
    "        'decay_factor': FLAGS.controller_decay_factor,\n",
    "        'attention': FLAGS.controller_attention,\n",
    "        'max_gradient_norm': FLAGS.controller_max_gradient_norm,\n",
    "        'time_major': FLAGS.controller_time_major,\n",
    "        'symmetry': FLAGS.controller_symmetry,\n",
    "        'predict_beam_width': FLAGS.controller_predict_beam_width,\n",
    "        'predict_lambda': FLAGS.controller_predict_lambda\n",
    "        \n",
    "    }\n",
    "    return params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "有一个log_variable_sizes是私有函数，但是没发现用的地方，暂时不写"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "真正的主体，train函数\n",
    "branch_length变量不明所以，branch-length有两种可能取值，2和3，当其为2的时候，就是正常的一个I对应一种operation，在controller认可的seq里就表现为两个值，但是当其为3的时候，却表示为三个值。具体为何这样，有待后面考究\n",
    "\n",
    "算法每次会从架构池里面随机采样出架构，一共有四种采样方式：\n",
    "\n",
    "1.均匀采样\n",
    "\n",
    "2.根据模型参数个数\n",
    "\n",
    "3.根据验证集上的表现\n",
    "\n",
    "4.根据预测的表现\n",
    "\n",
    "child_train(params)针对params里的架构进行采样训练，其中每一次迭代，都随机采样一个架构，然后架构上的边，根据随机batch的数据来更新对应的权重，感觉有点不太靠谱的样子。\n",
    "\n",
    "np.argsort(list)是用来返回一个list里数值从小到大的索引，放在上下文里，就是架构表现从好到差的索引\n",
    "\n",
    "fa fp分别保存这架构以及架构所对应的精度\n",
    "fa_latest和fp_latest保存这最新的架构和最新的精度\n",
    "\n",
    "eval_every_epochs 表示child训练多少个epoch就可以评价了\n",
    "encoder_target和decoder_target作用未知，可能是为了辅助loss—function？\n",
    "\n",
    "controller.train已经实现\n",
    "\n",
    "controller_params['predict_lambda']作用不明，看起来像一个计数器\n",
    "\n",
    "controller.predict(controller_params, top100_archs)原来是用来返回更新之后得到的架构，返回的是更新后的一堆架构，尚未实现，准备实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    child_params = get_child_model_params()\n",
    "    controller_params = get_controller_params()\n",
    "    branch_length = controller_params['source_length'] // 2 // 5 // 2\n",
    "    eval_every_epochs = child_params['eval_every_epochs']\n",
    "    child_epoch = 0\n",
    "    while True:\n",
    "        #train child model\n",
    "        if child_params['arch_pool'] is None:\n",
    "            arch_pool = utils.generate_arch(controller_params['num_seed_arch'],child_params['num_cells'],5 )\n",
    "            #[[[conv],[reduc]]]\n",
    "            child_params['arch_pool'] = arch_pool\n",
    "            child_params['arch_pool_prob'] = None\n",
    "        else:\n",
    "            if child_params['sample_policy'] == 'uniform':\n",
    "                child_params['arch_pool_prob'] = None\n",
    "            elif child_params['sample_policy'] == 'params':\n",
    "                child_params['arch_pool_prob'] = calculate_params(child_params['arch_pool'])\n",
    "            elif child_params['sample_policy'] == 'valid_performance':\n",
    "                child_params['arch_pool_prob'] = child_valid(child_params)\n",
    "            elif child_params['sample_policy'] == 'predicted_performance':\n",
    "                encoder_input = list(map(lambda x: utils.parse_arch_to_seq(x[0], branch_length)+utils.parse_arch_to_seq(x[1],branch_length), child_params['arch_pool'] ))\n",
    "                \n",
    "                predicted_error_rate = controller.test(controller_params, encoder_input)\n",
    "                child_params['arch_pool_prob'] = [1-i[0] for i in predicted_error_rate]\n",
    "            else:\n",
    "                raise ValueError('Child model arch pool sample policy is not provided!')\n",
    "        \n",
    "        if isinstance(eval_every_epochs, int):\n",
    "            child_params['eval_every_epochs'] = eval_every_epochs\n",
    "        else:\n",
    "            for index, e in enumerate(eval_every_epochs):\n",
    "                if child_epoch < e :\n",
    "                    child_params['eval_every_epochs'] = e\n",
    "                    break\n",
    "        child_epoch = child_train(child_params) #currently dont know what's this\n",
    "        \n",
    "        valid_accuracy_list = child_valid(child_params)\n",
    "        \n",
    "        old_archs = child_params['arch_pool']\n",
    "        old_archs_perf = [1 - i for i in valid_accuracy_list] # better accuary, less this value\n",
    "    \n",
    "        old_archs_sorted_indices = np.argsort(old_archs_perf)\n",
    "        old_archs = np.array(old_archs)[old_archs_sorted_indices].tolist()\n",
    "        old_archs_perf = np.array(old_archs_perf)[old_archs_sorted_indices].tolist()\n",
    "        with open(os.path.join(child_params['model_dir'], 'arch_pool.{}'.format(child_epoch)),'w') as fa:\n",
    "            with open(os.path.join(child_params['model_dir'], 'arch_pool.perf.{}'.format(child_epoch)),'w') as fp:\n",
    "                with open(os.path.join(child_params['model_dir'], 'arch_pool'),'w') as fa_latest:\n",
    "                    with open(os.path.join(child_params['model_dir'],'arch_pool.perf'),'w') as fp_latest:\n",
    "                        for arch, perf in zip(old_archs, old_archs_perf):\n",
    "                            arch = ' '.join(map(str, arch[0]+arch[1]))\n",
    "                            fa.write('{}\\n'.format(arch))\n",
    "                            fa_latest.write('{}\\n'.format(arch))\n",
    "                            fp.write('{}\\n'.format(perf))\n",
    "                            fp_latest.write('{}\\n'.format(perf))\n",
    "        if child_epoch >= child_params['num_epochs']:\n",
    "            break\n",
    "        \n",
    "        #train Encoder-Predictor-Decoder\n",
    "        encoder_input = list(map(lambda x: utils.parse_arch_to_seq(x[0],branch_length) + utils.parse_arch_to_seq(x[1], branch_length), old_archs ))\n",
    "        #[[conv, reduc]]\n",
    "        \n",
    "        min_val = min(old_archs_perf) #best\n",
    "        max_val = max(old_archs_perf) #worst\n",
    "        encoder_target = [(i - min_val) / (max_val - min_val) for i in old_archs_perf]\n",
    "        decoder_target = copy.copy(encoder_input)\n",
    "        controller_params['batches_per_epoch'] = math.ceil(len(encoder_input) / controller_params['batch_size'])\n",
    "        \n",
    "        #if clean controller model\n",
    "        controller.train(controller_params, encoder_input, encoder_target, decoder_target)\n",
    "        \n",
    "        #generate new archs\n",
    "        \n",
    "        new_archs = []\n",
    "        max_step_size = 100\n",
    "        controller_params['predict_lambda'] = 0\n",
    "        top100_archs = list(map(lambda x: utils.parse_arch_to_seq(x[0], branch_length)+utils.parse_arch_to_seq(x[1],branch_length), old_archs[:100]))\n",
    "        while len(new_archs) < 500:\n",
    "            controller_params['predict_lambda'] += 1\n",
    "            new_arch = controller.predict(controller_params, top100_archs)\n",
    "            \n",
    "            for arch in new_arch:\n",
    "                if arch not in encoder_input and arch not in new_archs:\n",
    "                    new_archs.append(arch)\n",
    "                    if len(new_archs) >=500:\n",
    "                        break\n",
    "            tf.logging.info('{} new archs generated now'.format(len(archs)))\n",
    "            if controller_params['predict_lambda'] > max_step_size:\n",
    "                break\n",
    "        new_archs = list(map(lambda x: utils.parse_seq_to_arch(x, branch_length), new_archs))\n",
    "        #[[[conv],[reduc]]]\n",
    "        num_new_archs = len(new_archs)\n",
    "        tf.logging.info('Generate {} new archs'.format(num_new_archs))\n",
    "        new_arch_pool = old_archs[:len(old_archs)-(num_new_archs+50)] + new_archs + utils.generate_arch(50,5,5)\n",
    "        tf.logging.info('Totally {} archs now to train'.format(len(new_arch_pool)))\n",
    "        \n",
    "        child_params['arch_pool'] = new_arch_pool\n",
    "        with open(os.path.join(child_params['model_dir'],'arch_pool'),'w') as f:\n",
    "            for arch in new_arch_pool:\n",
    "                arch = ' '.join(map(str, arch[0]+arch[1]))\n",
    "                f.write('{}\\n'.format(arch))\n",
    "            \n",
    "        \n",
    "                \n",
    "################Here!!!!!!!!!!!#################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "文件最末尾的main函数，os.environ['TF_ENABLE_WINOGRAD_NONFUSED'] = '1'用来加速训练（大家都这么说）, all_params是一个字典，key是所有参数--后面的字符串，value是对应的值，json.dump将参数存进这个json文件里了，做完之后，开始train()\n",
    "\n",
    "parser.parse_known_args()用来处理多余的参数，是保证鲁棒性？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(unused_argv):\n",
    "    os.environ['TF_ENABLE_WINOGRAD_NONFUSED'] = '1'\n",
    "    \n",
    "    all_params = vars(FLAGS)\n",
    "    with open(os.path.join(FLAGS.output_dir,'hparams.json'),'w') as f:\n",
    "        json.dump(all_params,f)\n",
    "    train()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    tf.logging.set_verbosity(tf.logging.INFO)\n",
    "    FLAGS, unparsed = parser.parse_known_args()\n",
    "    tf.app.run(argv=[sys.argv[0]+unparsed])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOS = 0\n",
    "\n",
    "def f():\n",
    "    a = tf.constant([SOS])\n",
    "    b = tf.constant([[1,2,3],[4,5,6]])\n",
    "    c = tf.concat([a,b[:-1]],axis=0)\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        print (sess.run(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Shape must be rank 1 but is rank 2 for 'concat_4' (op: 'ConcatV2') with input shapes: [1], [1,3], [].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-c43e34e6d405>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-12-841359b909e6>\u001b[0m in \u001b[0;36mf\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mSOS\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/array_ops.pyc\u001b[0m in \u001b[0;36mconcat\u001b[0;34m(values, axis, name)\u001b[0m\n\u001b[1;32m   1097\u001b[0m               tensor_shape.scalar())\n\u001b[1;32m   1098\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0midentity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1099\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mgen_array_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_concat_v2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_array_ops.pyc\u001b[0m in \u001b[0;36m_concat_v2\u001b[0;34m(values, axis, name)\u001b[0m\n\u001b[1;32m    704\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_graph_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    705\u001b[0m     _, _, _op = _op_def_lib._apply_op_helper(\n\u001b[0;32m--> 706\u001b[0;31m         \"ConcatV2\", values=values, axis=axis, name=name)\n\u001b[0m\u001b[1;32m    707\u001b[0m     \u001b[0m_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    708\u001b[0m     \u001b[0m_inputs_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.pyc\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    785\u001b[0m         op = g.create_op(op_type_name, inputs, output_types, name=scope,\n\u001b[1;32m    786\u001b[0m                          \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattr_protos\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 787\u001b[0;31m                          op_def=op_def)\n\u001b[0m\u001b[1;32m    788\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0moutput_structure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_stateful\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36mcreate_op\u001b[0;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_shapes, compute_device)\u001b[0m\n\u001b[1;32m   2956\u001b[0m         op_def=op_def)\n\u001b[1;32m   2957\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcompute_shapes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2958\u001b[0;31m       \u001b[0mset_shapes_for_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2959\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2960\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_record_op_seen_by_control_dependencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36mset_shapes_for_outputs\u001b[0;34m(op)\u001b[0m\n\u001b[1;32m   2207\u001b[0m       \u001b[0mshape_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_call_cpp_shape_fn_and_require_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2208\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2209\u001b[0;31m   \u001b[0mshapes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshape_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2210\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mshapes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2211\u001b[0m     raise RuntimeError(\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36mcall_with_requiring\u001b[0;34m(op)\u001b[0m\n\u001b[1;32m   2157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2158\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_with_requiring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2159\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcall_cpp_shape_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequire_shape_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2161\u001b[0m   \u001b[0m_call_cpp_shape_fn_and_require_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_with_requiring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/common_shapes.pyc\u001b[0m in \u001b[0;36mcall_cpp_shape_fn\u001b[0;34m(op, require_shape_fn)\u001b[0m\n\u001b[1;32m    625\u001b[0m     res = _call_cpp_shape_fn_impl(op, input_tensors_needed,\n\u001b[1;32m    626\u001b[0m                                   \u001b[0minput_tensors_as_shapes_needed\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 627\u001b[0;31m                                   require_shape_fn)\n\u001b[0m\u001b[1;32m    628\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m       \u001b[0;31m# Handles the case where _call_cpp_shape_fn_impl calls unknown_shape(op).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/common_shapes.pyc\u001b[0m in \u001b[0;36m_call_cpp_shape_fn_impl\u001b[0;34m(op, input_tensors_needed, input_tensors_as_shapes_needed, require_shape_fn)\u001b[0m\n\u001b[1;32m    689\u001b[0m       \u001b[0mmissing_shape_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 691\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    692\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    693\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mmissing_shape_fn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Shape must be rank 1 but is rank 2 for 'concat_4' (op: 'ConcatV2') with input shapes: [1], [1,3], []."
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1, 2),\n",
       " (1, 2),\n",
       " (1, 2),\n",
       " (1, 2),\n",
       " (1, 2),\n",
       " (1, 2),\n",
       " (1, 2),\n",
       " (1, 2),\n",
       " (1, 2),\n",
       " (1, 2))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = (1,2)\n",
    "b = (a,)*10\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 3, 5), (2, 4, 6)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [[1,2],[3,4],[5,6]]\n",
    "b = zip(*a)\n",
    "b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[1, 1],\n",
      "       [1, 1]], dtype=int32)]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'dtype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-354f89c87305>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mg_v\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mg_v\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0;32mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mg_v\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'dtype'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "a = tf.constant(1)\n",
    "b = tf.constant([[1,2],[3,4]])\n",
    "c = a * b\n",
    "g = tf.gradients(c,b)\n",
    "with tf.Session() as sess:\n",
    "    g_v = sess.run(g)\n",
    "    print (g_v)\n",
    "    print (g_v.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
