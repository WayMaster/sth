{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "引入一些模块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.training import moving_averages\n",
    "\n",
    "\n",
    "from data_utils import read_data\n",
    "from utils import count_model_params, get_train_ops\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从一个架构池中按照概率采样出一个架构，返回normal cell和reduction cell\n",
    "\n",
    "tf.squeeze表示去掉所有尺寸为1的维度\n",
    "\n",
    "tf.expand-dims(x,0)表示在x这个张量上添加一个维度0\n",
    "\n",
    "tf.multinomial()\n",
    "tf.multinomial(logits, num_samples, seed=None, name=None) \n",
    "logits 大小为[batch，n_class] \n",
    "num_samples 表示采样的个数 \n",
    "seed 随机种子数 \n",
    "name表示该op的名字\n",
    "\n",
    "这个函数就是根据logits中每个类别的概率采样，这个概率可以不用归一化后的概率，也就是每个类的概率可以大于1，不需要所有类别概率和为1，我觉得它底层是会自动给我们归一化的，输出结果为[batch,num_samples]大小 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_arch_from_pool(arch_pool, prob=None):\n",
    "    N = len(arch_pool)\n",
    "    arch_pool = tf.convert_to_tensor(arch_pool, dtype = tf.int32)\n",
    "    if prob is not None:\n",
    "        tf.logging.info(\"Arch pool prob is provided, sampling according to the prob\")\n",
    "        prob = tf.convert_to_tensor(prob, dtype=tf.float32)\n",
    "        prob = tf.expand_dims(tf.squeeze(prob),axis=0)\n",
    "        index = tf.multinomial(prob,1)[0][0]\n",
    "    else:\n",
    "        index = tf.random_uniform([],minval=0, maxval=N, dtype=tf.int32)\n",
    "    arch = arch_pool[index]\n",
    "    conv_arch = arch[0]\n",
    "    reduc_arch = arch[1]\n",
    "    return conv_arch, reduc_arch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "创建给定名字形状和初始方式的变量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_weight(name, shape, initializer = None, trainable = True, seed=None):\n",
    "    if initializer is None:\n",
    "        initializer = tf.contrib.keras.initializers.he_normal(seed = seed)\n",
    "    return tf.get_variable(name, shape, initializer = initializer, trainable=trainable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_weight(name, shape, initializer= None, trainable=True, seed=None):\n",
    "    if initializer is None:\n",
    "        initializer = tf.contrib.keras.initializers.he_normal(seed = seed)\n",
    "    \n",
    "    w1 = tf.get_variable(name+\"dag_1\", shape, initializer=initializer, trainable=trainable)\n",
    "    w2 = tf.get_variable(name+\"dag_2\", shape, initializer=initializer, trainable=trainable)\n",
    "    a = tf.get_variable(\"dag_manage\",shape=[2], initializer=initializer, trainable=trainable)\n",
    "    a = tf.nn.softmax(a)\n",
    "    w = tf.add(tf.multiply(w1,a[0]),tf.multiply(w2,a[1]))\n",
    "    \n",
    "    return w\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2,)\n",
      "[-0.34195682  0.91937125]\n"
     ]
    }
   ],
   "source": [
    "a = tf.constant(0)\n",
    "a = tf.get_variable('aaaaa',shape=[2],initializer=tf.contrib.keras.initializers.he_normal())\n",
    "with tf.Session() as sess:\n",
    "    tf.initialize_all_variables().run()\n",
    "    a_v = sess.run(a)\n",
    "    print (a_v.shape)\n",
    "    print (a_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 3.5  4.5  5.5]\n"
     ]
    }
   ],
   "source": [
    "a = tf.constant([1.0,1.0])\n",
    "w1 = tf.constant([2.0,3.0,4.0])\n",
    "w2 = tf.constant([5.0,6.0,7.0])\n",
    "a = tf.nn.softmax(a)\n",
    "w = tf.multiply(a[0],w1) + tf.multiply(w2,a[1])\n",
    "with tf.Session() as sess:\n",
    "    print (sess.run(w))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于一个batch里的数据来说，按照给定概率丢弃一些样本求出的数据，剩余样本的各个分量都除以给定的概率，以保证期望不变，给定概率越大，留存下来的样本数就越多"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_path(x, keep_prob):\n",
    "    batch_size = tf.shape(x)[0]\n",
    "    noise_shape = [batch_size,1,1,1]\n",
    "    random_tensor = keep_prob\n",
    "    random_tensor += tf.random_uniform(noise_shape, dtype=tf.float32)\n",
    "    binary_tensor = tf.floor(random_tensor)\n",
    "    x = tf.div(x, keep_prob) * binary_tensor\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一个NHWC类型的向量变成 [H,1,1,C] 即将一个图所有位置求平均，缩成一个点"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def global_avg_pool(x, data_format = \"NHWC\"):\n",
    "    if data_format == \"NHWC\":\n",
    "        x = tf.reduce_mean(x, [1,2])\n",
    "    elif data_format == \"NCHW\":\n",
    "        x = tf.reduce_mean(x, [2,3])\n",
    "    else:\n",
    "        raise NotImplementedError(\"Unknown data_format {}\".format(data_format))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "几乎在所有的卷积层之后都添加了这样一个操作，这也正是之前计算参数个数里面多出来的疑似1x1卷积的操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_norm(x, is_training, name=\"bn\", decay=0.9, epilson=1e-5, data_format=\"NHWC\"):\n",
    "    if data_format == \"NHWC\":\n",
    "        shape = [x.get_shape()[3]]\n",
    "    elif data_format == \"NCHW\":\n",
    "        shape = [x.get_shape()[1]]\n",
    "    else:\n",
    "        raise NotImplementedError(\"Unknown data_format {}\".format(data_format))\n",
    "    \n",
    "    with tf.variable_scope(name, reuse = tf.AUTO_REUSE):\n",
    "        offset = tf.get_variable(\"offset\", shape, initializer = tf.constant_initializer(0.0, dtype=tf.float32))\n",
    "        scale = tf.get_variable(\"scale\", shape, initializer = tf.constant_initializer(1.0, dtype=tf.float32))\n",
    "        moving_mean = tf.get_variable(\"moving_mean\", shape, trainable=False, initializer = tf.constant_initializer(0.0, dtype=tf.float32))\n",
    "        moving_variance = tf.get_variable(\"moving_variance\", shape, trainable=False, initializer = tf.constant_initializer(1.0, dtype=tf.float32))\n",
    "        \n",
    "        if is_training:\n",
    "            x, mean, variance = tf.nn.fused_batch_norm(x, scale, offset, epsilon=epsilon, data_format=data_format, is_training=True)\n",
    "            update_mean = moving_averages.assign_moving_average(moving_mean, mean, decay)\n",
    "            update_variance = moving_averages.assign_moving_average(moving_variance, variance, decay)\n",
    "            with tf.control_dependencies([update_mean, update_variance]):\n",
    "                x = tf.identity(x)\n",
    "        else:\n",
    "            x, _, _ = tf.nn.fused_batch_norm(x, scale, offset, mean=moving_mean, variance=moving_variance, epsilon=epsilon, data_format=data_format, is_training=False)\n",
    "            \n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "relu激活函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x, leaky=0.0):\n",
    "    return tf.where(tf.greater(x,0),x,x*leaky)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "定义一个model类\n",
    "\n",
    "其中-pre-process对输入的图像进行一个预处理，先pad上下左右4个空格，然后进行随机裁剪，随机翻转，按照cut_out_size进行处理\n",
    "\n",
    "_build_valid 函数根据保存在self.x_valid和self.y_valid以及其中的normal_arc和reduce_arc给出相应的验证集上的精度\n",
    "\n",
    "_model 函数就是用来执行给定的神经网络，输入x，输出预测的logits\n",
    "\n",
    "_enas_layer根据输入的边的链接方式执行一个cell\n",
    "\n",
    "_maybe_calibrate_size用来将两个输入处理成同样的H和W，以及channel个数\n",
    "\n",
    "_factorized_reduction 不减少信息量，增加channel为两倍\n",
    "\n",
    "_enas_cell 用于计算一对（I，op）\n",
    "\n",
    "fixed-arc的意思其实是固定卷积层，常规情况下，对于每一对I,OP都需要准备相应的参数，不同的输入加卷积操作，都要设定一个参数，但是在fix的情况下，所有卷积operation的参数是相同的，也就是说，对于不同的I，如果后面的操作相同，那么卷积层使用的参数是相同的。(好像区别不在这里，因为后来经检验发现，常规情况，对于不同的I，如果后面的操作相同，那么卷积层使用的参数，也是相同的)\n",
    "\n",
    "\n",
    "—fixed-layer函数有一个奇怪的layer-base，即将上一层的输入，进行一个1x1卷积的操作，动机未知; 除此之外，fixed——arc和普通arc最后整合输出的时候也略有不同，fixed-arc直接concat一起，而普通arc进行了一个1x1的卷积操作，因此fixed-arc输出的并不是out_filters数量的通道数，而且更多的，这点如何处理，目前未知。12.26.9：47！！！！！！！！！！！！！！！！ok，明白了，原来之前的1x1的卷积操作，就是为了处理刚刚最后所说的fix-arc输出的通道数不统一的情况，至于为何写的不统一，未知，总之，fix-arc和普通arc在这里的处理其实是一样的\n",
    "\n",
    "fix-arc中保存的竟然是一个完整的架构！！！！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(object):\n",
    "    def __init__(self,\n",
    "                images,\n",
    "                labels,\n",
    "                use_aux_heads=False,\n",
    "                cutout_size=None,\n",
    "                fixed_arc=None,\n",
    "                num_layers=2,\n",
    "                num_cells=5,\n",
    "                out_filters=24,\n",
    "                keep_prob=1.0,\n",
    "                drop_path_keep_prob=None,\n",
    "                batch_size=32,\n",
    "                eval_batch_size=100,\n",
    "                clip_mode=None,\n",
    "                grad_bound=None,\n",
    "                l2_reg=1e-4,\n",
    "                lr_init=0.1,\n",
    "                lr_dec_start=0,\n",
    "                lr_dec_every=10000,\n",
    "                lr_dec_rate=0.1,\n",
    "                lr_cosine=False,\n",
    "                lr_max=None,\n",
    "                lr_min=None,\n",
    "                lr_T_0=None,\n",
    "                lr_T_mul=None,\n",
    "                num_epochs=None,\n",
    "                optim_algo=None,\n",
    "                sync_replicas=False,\n",
    "                num_aggregate=None,\n",
    "                num_replicas=None,\n",
    "                data_format=\"NHWC\",\n",
    "                name=\"child\",\n",
    "                seed=None,\n",
    "                baseline=0.0,\n",
    "                **kwargs\n",
    "                ):\n",
    "        tf.logging.info(\"-\" * 80)\n",
    "        tf.logging.info(\"Build model {}\".format(name))\n",
    "        \n",
    "        self.cutout_size = cutout_size\n",
    "        self.batch_size = batch_size\n",
    "        self.eval_batch_size = eval_batch_size\n",
    "        self.clip_mode = clip_mode\n",
    "        self.grad_bound = grad_bound\n",
    "        self.l2_reg = l2_reg\n",
    "        self.lr_init = lr_init\n",
    "        self.lr_dec_start = lr_dec_start\n",
    "        self.lr_dec_rate = lr_dec_rate\n",
    "        self.keep_prob = keep_prob\n",
    "        self.optim_algo = optim_algo\n",
    "        self.sync_replicas = sync_replicas\n",
    "        self.num_aggregate = num_aggregate\n",
    "        self.num_replicas = num_replicas\n",
    "        self.data_format = data_format\n",
    "        self.name = name\n",
    "        self.seed = seed\n",
    "        \n",
    "        self.global_step = None\n",
    "        self.valid_acc = None\n",
    "        self.test_acc = None\n",
    "        tf.logging.info(\"Build data ops\")\n",
    "        \n",
    "        with tf.device(\"/cpu:0\"):\n",
    "            self.num_train_examples = np.shape(images[\"train\"])[0]\n",
    "            self.num_train_batches = (self.num_train_examples + self.batch_size - 1) // self.batch_size\n",
    "            \n",
    "            x_train, y_train = tf.train.shuffle_batch(\n",
    "                [images[\"train\"], labels[\"train\"]],\n",
    "                batch_size = self.batch_size,\n",
    "                capacity = 50000,\n",
    "                enqueue_many = True,\n",
    "                min_after_dequeue = 0,\n",
    "                num_threads = 16,\n",
    "                seed = self.seed,\n",
    "                allow_smaller_final_batch = True,\n",
    "            )\n",
    "            self.lr_dec_every = lr_dec_every * self.num_train_batches\n",
    "            \n",
    "            def _pre_process(x):\n",
    "                x = tf.pad(x,[[4,4],[4,4],[0,0]])\n",
    "                x = tf.random_crop(x, [32,32,3], seed = self.seed)\n",
    "                x = tf.image.random_flip_left_right(x, seed = self.seed)\n",
    "                if self.cutout_size is not None:\n",
    "                    mask = tf.ones([self.cutout_size, self.cutout_size], dtype=tf.int32)\n",
    "                    start = tf.random_uniform([2], minval=0, maxval=32, dtype=tf.int32)\n",
    "                    mask = tf.pad(mask, [[self.cutout_size + start[0], 32 - start[0]], \n",
    "                                         [self.cutout_size + start[1], 32 - start[1]]])\n",
    "                    mask = mask[self.cutout_size: self.cutout_size + 32, self.cutout_size: self.cutout_size + 32]\n",
    "                    mask = tf.reshape(mask, [32,32,1])\n",
    "                    mask = tf.tile(mask, [1,1,3])\n",
    "                    x = tf.where(tf.equal(mask, 0), x=x, y=tf.zeros_like(x))\n",
    "                if self.data_format == \"NCHW\":\n",
    "                    x = tf.transpose(x,[2, 0, 1])\n",
    "                \n",
    "                return x\n",
    "            \n",
    "            self.x_train = tf.map_fn(_pre_process, x_train, back_prop = False)\n",
    "            self.y_train = y_train\n",
    "            \n",
    "            self.x_valid, self.y_valid = None, None\n",
    "            \n",
    "            if images[\"valid\"] is not None:\n",
    "                images[\"valid_original\"] = np.copy(images[\"valid\"])\n",
    "                labels[\"valid_original\"] = np.copy(labels[\"valid\"])\n",
    "                if self.data_format == \"NCHW\":\n",
    "                    images[\"valid\"] = tf.transpose(images[\"valid\"], [0,3,1,2])\n",
    "                self.num_valid_examples = np.shape(images[\"valid\"])[0]\n",
    "                self.num_valid_batches = ((self.num_valid_examples + self.eval_batch_size - 1) // self.eval_batch_size)\n",
    "                self.x_valid, self.y_valid = tf.train.batch(\n",
    "                    [images[\"valid\"], labels[\"valid\"]],\n",
    "                    batch_size = self.eval_batch_size,\n",
    "                    capacity = 5000,\n",
    "                    enqueue_many = True,\n",
    "                    num_threads = 1,\n",
    "                    allow_smaller_final_batch = True,\n",
    "                )\n",
    "            \n",
    "            if self.data_format == \"NCHW\":\n",
    "                images[\"test\"] = tf.transpose(images[\"test\"], [0,3,1,2])\n",
    "            self.num_test_examples = np.shape(images[\"test\"])[0]\n",
    "            self.num_test_batches = ((self.num_test_examples + self.eval_batch_size - 1) // self.eval_batch_size)\n",
    "            self.x_test, self.y_test = tf.train.batch(\n",
    "                [images[\"test\"], labels[\"test\"]],\n",
    "                batch_size = self.eval_batch_size,\n",
    "                capacity = 10000,\n",
    "                enqueue_many = True,\n",
    "                num_threads = 1,\n",
    "                allow_smaller_final_batch = True,\n",
    "            )\n",
    "        \n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        \n",
    "        if self.data_format == \"NHWC\":\n",
    "            self.actual_data_format = \"channels_last\"\n",
    "        elif self.data_format == \"NCHW\":\n",
    "            self.actual_data_format = \"channels_first\"\n",
    "        else:\n",
    "            raise ValueError(\"Unknown data_format '{0}'\".format(self.data_format))\n",
    "        \n",
    "        self.use_aux_heads = use_aux_heads\n",
    "        self.num_epochs = num_epochs\n",
    "        self.num_train_steps = self.num_epochs * self.num_train_batches\n",
    "        self.drop_path_keep_prob = drop_path_keep_prob\n",
    "        self.lr_cosine = lr_cosine\n",
    "        self.lr_max = lr_max\n",
    "        self.lr_min = lr_min\n",
    "        self.lr_T_0 = lr_T_0\n",
    "        self.lr_T_mul = lr_T_mul\n",
    "        self.out_filters = out_filters\n",
    "        self.num_layers = num_layers\n",
    "        self.num_cells = num_cells\n",
    "        self.fixed_arc = fixed_arc\n",
    "        \n",
    "        with tf.variable_scope(name, reuse = tf.AUTO_REUSE):\n",
    "            self.global_step = tf.train.get_or_create_global_step()\n",
    "        \n",
    "        if self.drop_path_keep_prob is not None:\n",
    "            assert num_epochs is not None, \"Need num_epochs to drop_path\"\n",
    "        \n",
    "        pool_distance = self.num_layers // 3 \n",
    "        self.pool_layers = [pool_distance, 2 * pool_distance + 1]\n",
    "        \n",
    "        if self.use_aux_heads:\n",
    "            self.aux_head_indices = [self.pool_layers[-1] + 1]\n",
    "        \n",
    "    def eval_once(self, sess, eval_set, feed_dict=None, verbose=False):\n",
    "        '''\n",
    "        Expects self.acc and self.global_step to be defined.\n",
    "        \n",
    "        Args:\n",
    "            sess: tf.Session() or one of its wrap arounds\n",
    "            feed_dict: can be used to give more information to sess.run()\n",
    "            eval_set: 'valid' or test\n",
    "        \n",
    "        '''\n",
    "        assert self.global_step is not None\n",
    "        global_step = sess.run(self.global_step)\n",
    "        tf.logging.info('Eval at {}'.format(global_step))\n",
    "        \n",
    "        if eval_set == 'valid':\n",
    "            assert self.x_valid is not None\n",
    "            assert self.valid_acc is not None\n",
    "            num_examples = self.num_valid_examples\n",
    "            num_batches = self.num_valid_batches\n",
    "            acc_op = self.valid_acc\n",
    "        elif eval_set == \"test\":\n",
    "            assert self.test_acc is not None\n",
    "            num_examples = self.num_test_examples\n",
    "            num_batches = self.num_test_batches\n",
    "            acc_op = self.test_acc\n",
    "        else:\n",
    "            raise NotImplementedError(\"Unknown eval_set '{}' \".format(eval_set))\n",
    "        total_acc = 0\n",
    "        total_exp = 0\n",
    "        for batch_id in range(num_batches):\n",
    "            acc = sess.run(acc_op, feed_dict = feed_dict)\n",
    "            total_acc += acc\n",
    "            total_exp += self.eval_batch_size\n",
    "            if verbose:\n",
    "                sys.stdout.write(\"\\r{:<5d}/{:>5d}\".format(total_acc,total_exp))\n",
    "        if verbose:\n",
    "            tf.logging.info(\"\")\n",
    "        tf.logging.info(\"{}_accuracy: {:<6.4f}\".format(eval_set, float(total_acc)/total_exp))\n",
    "        \n",
    "    def _get_C(self, x):\n",
    "        if self.data_format == \"NHWC\":\n",
    "            return x.get_shape()[3].value\n",
    "        elif self.data_format == \"NCHW\":\n",
    "            return x.get_shape()[1].value\n",
    "        else:\n",
    "            raise ValueError(\"Unknown data_format '{0}'\".format(self.data_format))\n",
    "    def _get_HW(self, x):\n",
    "        return x.get_shape()[2].value\n",
    "    \n",
    "    def _get_strides(self, stride):\n",
    "        if self.data_format == \"NHWC\":\n",
    "            return [1,stride,stride,1]\n",
    "        elif self.data_format == \"HCHW\":\n",
    "            return [1,1,stride,stride]\n",
    "        else:\n",
    "            raise ValueError(\"Unknown data_format '{}'\".format(self.data_format))\n",
    "    \n",
    "    def _factorized_reduction(self, x, out_filters, stride, is_training):\n",
    "        assert out_filters % 2 == 0, (\"Need even number of filters when using this factorized reduction\")\n",
    "        \n",
    "        if stride == 1:\n",
    "            with tf.variable_scope(\"path_conv\"):\n",
    "                inp_c = self._get_C(x)\n",
    "                w = create_weight(\"w\", [1,1,inp_c,out_filters])\n",
    "                x = tf.nn.conv2d(x,w,[1,1,1,1],\"SAME\", data_format=self.data_format)\n",
    "                x = batch_norm(x,is_training,data_format=self.data_format)\n",
    "                return x\n",
    "        stride_spec = self._get_strides(stride)\n",
    "        path1 = tf.nn.avg_pool(x, [1,1,1,1],stride_spec,'VALID',data_format=self.data_format)\n",
    "        with tf.variable_scope(\"path1_conv\"):\n",
    "            inp_c = self._get_C(path1)\n",
    "            w = create_weight(\"w\", [1,1,inp_c,out_filters // 2])\n",
    "            path1 = tf.nn.conv2d(path1,w,[1,1,1,1],\"VALID\",data_format=self.data_format)\n",
    "        \n",
    "        if self.data_format == \"NHWC\":\n",
    "            pad_arr = [[0,0],[0,1],[0,1],[0,0]]\n",
    "            path2 = tf.pad(x,pad_arr)[:,1:,1:,:]\n",
    "            concat_axis = 3\n",
    "        else:\n",
    "            pad_arr = [[0,0],[0,0],[0,1],[0,1]]\n",
    "            path2 = tf.pad(x,pad_arr)[:,:,1:,1:]\n",
    "            concat_axis = 1\n",
    "        path2 = tf.nn.avg_pool(path2, [1,1,1,1], stride_spec, \"VALID\",data_format=self.data_format)\n",
    "        with tf.variable_scope(\"path2_conv\"):\n",
    "            inp_c = self._get_C(path2)\n",
    "            w = create_weight(\"w\",[1,1,inp_c,out_filters // 2])\n",
    "            path2 = tf.nn.conv2d(path2,w,[1,1,1,1],\"VALID\",data_format=self.data_format)\n",
    "        \n",
    "        final_path = tf.concat(values=[path1,path2], axis=concat_axis)\n",
    "        final_path = batch_norm(final_path,is_training,data_format=self.data_format)\n",
    "        \n",
    "        return final_path\n",
    "    \n",
    "    def _apply_drop_path(self, x, layer_id):\n",
    "        \n",
    "        drop_path_keep_prob = self.drop_path_keep_prob\n",
    "        layer_ratio = float(layer_id + 1) / (self.num_layers + 2)\n",
    "        drop_path_keep_prob = 1 - layer_ratio * (1 - drop_path_keep_prob)\n",
    "        step_ratio = tf.to_float(self.global_step + 1) / tf.to_float(self.num_train_steps)\n",
    "        step_ratio = tf.minimum(1.0, step_ratio)\n",
    "        drop_path_keep_prob = 1 - layer_ratio * (1 - drop_path_keep_prob)\n",
    "        \n",
    "        x = drop_path(x, drop_path_keep_prob)\n",
    "        return x\n",
    "    \n",
    "    def _maybe_calibrate_size(self, layers, out_filters, is_training):\n",
    "        \n",
    "        hw = [self._get_HW(layer) for layer in layers]\n",
    "        c = [self._get_C(layer) for layer in layers]\n",
    "        \n",
    "        with tf.variable_scope(\"calibrate\"):\n",
    "            x = layers[0]\n",
    "            if hw[0] != hw[1]:\n",
    "                assert hw[0] == 2 * hw[1]\n",
    "                with tf.variable_scope(\"pool_x\"):\n",
    "                    x = tf.nn.relu(x)\n",
    "                    x = self._factorized_reduction(x, out_filters, 2, is_training)\n",
    "            elif c[0]!= out_filters:\n",
    "                with tf.variable_scope(\"pool_x\"):\n",
    "                    x = tf.nn.relu(x)\n",
    "                    w = create_weight(\"w\",[1,1,c[0],out_filters])\n",
    "                    x = tf.nn.conv2d(x,w,[1,1,1,1],\"SAME\",data_format=self.data_format)\n",
    "                    x = batch_norm(x,is_training,data_format=self.data_format)\n",
    "            \n",
    "            y = layers[1]\n",
    "            if c[1] != out_filters:\n",
    "                with tf.variable_scope(\"pool_y\"):\n",
    "                    y = tf.nn.relu(y)\n",
    "                    w = create_weight(\"w\",[1,1,c[1],out_filters])\n",
    "                    y = tf.nn.conv2d(y,w,[1,1,1,1],\"SAME\",data_format=self.data_format)\n",
    "                    y = batch_norm(y, is_training, data_format= self.data_format)\n",
    "        \n",
    "        return [x, y]\n",
    "    \n",
    "    def _model(self, images, is_training, reuse=tf.AUTO_REUSE):\n",
    "        \n",
    "        if self.fixed_arc is None:\n",
    "            is_training = True\n",
    "        \n",
    "        with tf.variable_scope(self.name, reuse=reuse):\n",
    "            # the first two inputs\n",
    "            with tf.variable_scope(\"stem_conv\"):\n",
    "                w = create_weight(\"w\",[3,3,3, self.out_filters * 3])\n",
    "                x = tf.nn.conv2d(images, w, [1,1,1,1],\"SAME\",data_format = self.data_format)\n",
    "                x = batch_norm(x, is_training, data_format = self.data_format)\n",
    "            if self.data_format == \"NHWC\":\n",
    "                split_axis = 3\n",
    "            elif self.data_format == \"NCHW\":\n",
    "                split_axis = 1\n",
    "            else:\n",
    "                raise ValueError(\"Unknown data_format '{0}'\".format(self.data_format))\n",
    "            \n",
    "            layers = [x, x]\n",
    "            \n",
    "            #building layers in the micro space\n",
    "            out_filters = self.out_filters\n",
    "            for layer_id in range(self.num_layers + 2):\n",
    "                with tf.variable_scope(\"layer_{0}\".format(layer_id)):\n",
    "                    if layer_id not in self.pool_layers:\n",
    "                        if self.fixed_arc is None:\n",
    "                            x = self._enas_layer(layer_id, layers, self.normal_arc, out_filters)                           \n",
    "                        else:\n",
    "                            x = self._fixed_layer(layer_id, layers, self.normal_arc, out_filters, 1, is_training, normal_or_reduction_cell=\"normal\")\n",
    "                    else:\n",
    "                        out_filters *= 2\n",
    "                        if self.fixed_arc is None:\n",
    "                            x = self._factorized_reduction(x, out_filters, 2, is_training)\n",
    "                            layers = [layers[-1], x]\n",
    "                            x = self._enas_layer(layer_id, layers, self.reduce_arc, out_filters)\n",
    "                        else:\n",
    "                            x = self._fixed_layer(layer_id, layers, self.reduce_arc, out_filters, 2, is_training, normal_or_reduction_cell=\"reduction\")\n",
    "                    tf.logging.info(\"Layer {0:>2d}: {1}\".format(layer_id, x))\n",
    "                    layers = [layers[-1], x]\n",
    "                \n",
    "                self.num_aux_vars = 0\n",
    "                if (self.use_aux_heads and layer_id in self.aux_head_indices and is_training):\n",
    "                    tf.logging.info(\"Using aux_head at layer {0}\".format(layer_id))\n",
    "                    with tf.variable_scope(\"aux_head\"):\n",
    "                        aux_logits = tf.nn.relu(x)\n",
    "                        aux_logits = tf.layers.average_pooling2d(aux_logits, [5,5], [3,3], \"VALID\", data_format=self.actual_data_format)\n",
    "                        with tf.variable_scope(\"proj\"):\n",
    "                            inp_c = self._get_C(aux_logits)\n",
    "                            w = create_weight(\"w\", [1, 1, inp_c, 128])\n",
    "                            aux_logits = tf.nn.conv2d(aux_logits, w, [1,1,1,1], \"SAME\",data_format=self.data_format)\n",
    "                            aux_logits = batch_norm(aux_logits, is_training=True, data_format=self.data_format)\n",
    "                            aux_logits = tf.nn.relu(aux_logits)\n",
    "                            \n",
    "                        with tf.variable_scope(\"avg_pool\"):\n",
    "                            inp_c = self._get_C(aux_logits)\n",
    "                            hw = self._get_HW(aux_logits)\n",
    "                            w = create_weight(\"w\",[hw,hw,inp_c,768])\n",
    "                            aux_logits = tf.nn.conv2d(aux_logits,w,[1,1,1,1],\"SAME\",data_format=self.data_format)\n",
    "                            aux_logits = batch_norm(aux_logits, is_training = True, data_format=self.data_format)\n",
    "                            aux_logits = tf.nn.relu(aux_logits)\n",
    "                        with tf.variable_scope(\"fc\"):\n",
    "                            aux_logits = global_avg_pool(aux_logits, data_format=self.data_format)\n",
    "                            inp_c = aux_logits.get_shape()[1].value\n",
    "                            w = create_weight(\"w\", [inp_c, 10])\n",
    "                            aux_logits = tf.matmul(aux_logits, w)\n",
    "                            self.aux_logits = aux_logits\n",
    "                    aux_head_variables = [ var for var in tf.trainable_variables() if var.name.startswith(self.name) and \"aux_head\" in var.name ]\n",
    "                    self.num_aux_vars = count_model_params(aux_head_variables)\n",
    "                    tf.logging.info(\"Aux head uses {0} params\".format(self.num_aux_vars))\n",
    "            x = tf.nn.relu(x)\n",
    "            x = global_avg_pool(x, data_format=self.data_format)\n",
    "            if is_training and self.keep_prob is not None and self.keep_prob < 1.0:\n",
    "                x = tf.nn.dropout(x, self.keep_prob)\n",
    "            with tf.variable_scope(\"fc\"):\n",
    "                inp_c = self._get_C(x)\n",
    "                w = create_weight(\"w\", [inp_c, 10])\n",
    "                x = tf.matmul(x, w)\n",
    "        return x\n",
    "                                            \n",
    "    \n",
    "    def _fixed_conv(self, x, f_size, out_filters, stride, is_training, stack_convs = 2):\n",
    "        \n",
    "        for conv_id in range(stack_convs):\n",
    "            inp_c = self._get_C(x)\n",
    "            if conv_id == 0:\n",
    "                strides = self._get_strides(stride)\n",
    "            else:\n",
    "                strides = [1,1,1,1]\n",
    "            with tf.variable_scope(\"sep_conv_{}\".format(conv_id)):\n",
    "                w_depthwise = create_weight(\"w_depth\",[f_size,f_size, inp_c, 1])\n",
    "                w_pointwise = create_weight(\"w_point\",[1,1, inp_c, out_filters])\n",
    "                x = tf.nn.relu(x)\n",
    "                x = tf.nn.separable_conv2d(x,depthwise_filter=w_depthwise,pointwise_filter=w_pointwise,strides=strides,padding=\"SAME\",data_format=self.data_format)\n",
    "                x = batch_norm(x,is_training=is_training,data_format=self.data_format)\n",
    "        return x\n",
    "    \n",
    "    def _fixed_combine(self, layers, used, out_filters, is_training, normal_or_reduction_cell=\"normal\"):\n",
    "        out_hw = min([self._get_HW(layer) for i,layer in enumerate(layers if used[i] == 0)])\n",
    "        out = []\n",
    "        \n",
    "        with tf.variable_scope(\"final_combine\"):\n",
    "            for i, layer in enumerate(layers):\n",
    "                if used[i] == 0:\n",
    "                    hw = self._get_HW(layer)\n",
    "                    if hw > out_hw:\n",
    "                        assert hw == out_hw * 2, (\"i_hw={0} != {1}=o_hw\".format(hw, out_hw))\n",
    "                        with tf.variable_scope(\"calibrate_{0}\".format(i)):\n",
    "                            x = self._factorized_reduction(layer, out_filters, 2, is_training)\n",
    "                    else:\n",
    "                        x = layer\n",
    "                    out.append(x)\n",
    "            if self.data_format == \"NHWC\":\n",
    "                out = tf.concat(out, axis=3)\n",
    "            elif self.data_format == \"NCHW\":\n",
    "                out = tf.concat(out, axis=1)\n",
    "            else:\n",
    "                raise ValueError(\"Unknown data_format '{0}'\".format(self.data_format))\n",
    "        return out\n",
    "    \n",
    "    def _fixed_layer(self, layer_id, prev_layers, arc, out_filters, stride, is_training, normal_or_reduction_cell=\"normal\"):\n",
    "        '''\n",
    "        prev_layers: cache or previous layers. for skip connection\n",
    "        is_training: for batch_norm\n",
    "        '''\n",
    "        assert len(prev_layers) == 2\n",
    "        layers = [prev_layers[0], prev_layers[1]]\n",
    "        layers = self._maybe_calibrate_size(layers, out_filters, is_training = is_training)\n",
    "        \n",
    "        with tf.variable_scope(\"layer_base\"):\n",
    "            x = layers[1]\n",
    "            inp_c = self._get_C(x)\n",
    "            w = create_weight(\"w\",[1,1,inp_c,out_filters])\n",
    "            x = tf.nn.relu(x)\n",
    "            x = tf.nn.conv2d(x,w,strides = [1,1,1,1],padding=\"SAME\",data_format=self.data_format)\n",
    "            x = batch_norm(x,is_training=is_training,data_format=self.data_format)\n",
    "            layers[1] = x\n",
    "        \n",
    "        used = np.zeros([self.num_cells + 2], dtype = np.int32)\n",
    "        f_sizes = [3, 5]\n",
    "        for cell_id in range(self.num_cells):\n",
    "            with tf.variable_scope(\"cell_{}\".format(cell_id)):\n",
    "                x_id = arc[4 * cell_id]\n",
    "                used[x_id] += 1\n",
    "                x_op = arc[4 * cell_id + 1]\n",
    "                x = layers[x_id]\n",
    "                x_stride = stride if x_id in [0, 1] else 1\n",
    "                with tf.variable_scope(\"x_conv\"):\n",
    "                    if x_op in [0, 1]:\n",
    "                        f_size = f_sizes[x_op]\n",
    "                        x = self._fixed_conv(x, f_size, out_filters, x_stride, is_training)\n",
    "                    \n",
    "                    elif x_op in [2, 3]:\n",
    "                        if x_op == 2:\n",
    "                            x = tf.layers.average_pooling2d(x, [3,3],[x_stride,x_stride],\"SAME\",data_format=self.actual_data_format)\n",
    "                        else:\n",
    "                            x = tf.layers.max_pooling2d(x, [3,3],[x_stride,x_stride ], \"SAME\",data_format=self.actual_data_format)\n",
    "                        if inp_c != out_filters:\n",
    "                            w = create_weight(\"w\",[1,1,inp_c,out_filters])\n",
    "                            x = tf.nn.relu(x)\n",
    "                            x = tf.nn.conv2d(x,w,strides = [1,1,1,1],padding=\"SAME\",data_format=self.data_format)\n",
    "                            x = batch_norm(x, is_training = is_training, data_format= self.data_format)\n",
    "                    else:\n",
    "                        \n",
    "                        if x_stride >1:\n",
    "                            assert x_stride == 2\n",
    "                            x = self._factorized_reduction(x,out_filters, stride = x_stride, is_training=is_training)\n",
    "                        inp_c = self._get_C(x)\n",
    "                        if inp_c != out_filters:\n",
    "                            w = create_weight(\"w\",[1,1,inp_c,out_filters])\n",
    "                            x = tf.nn.relu(x)\n",
    "                            x = tf.nn.conv2d(x, w, strides=[1,1,1,1],padding=\"SAME\",data_format=self.data_format)\n",
    "                            x = batch_norm(x, is_training=is_training,data_format=self.data_format)\n",
    "                    if (x_op in [0,1,2,3] and self.drop_path_keep_prob is not None and is_training):\n",
    "                        x = self._apply_drop_path(x, layer_id)\n",
    "                y_id = arc[4*cell_id + 2]\n",
    "                y_op = arc[4*cell_id + 3]\n",
    "                used[y_id] += 1\n",
    "                y = layers[y_id]\n",
    "                y_stride = stride if y_id in [0,1] else 1\n",
    "                with tf.variable_scope(\"y_conv\"):\n",
    "                    if y_op in [0,1]:\n",
    "                        f_size = f_sizes[y_op]\n",
    "                        y = self._fixed_conv(y, f_size,out_filters, y_stride,is_training=is_training)\n",
    "                    elif y_op in [2,3]:\n",
    "                        inp_c = self._get_C(y)\n",
    "                        if y_op == 2:\n",
    "                            y = tf.layers.average_pooling2d(y,[3,3],[y_stride,y_stride],padding=\"SAME\",data_format=self.actual_data_format)\n",
    "                        else:\n",
    "                            y = tf.layers.max_pooling2d(y, [3,3],[y_stride,y_stride], padding=\"SAME\",data_format= self.actual_data_format)\n",
    "                        if inp_c != out_filters:\n",
    "                            w = create_weight(\"w\",[1,1,inp_c,out_filters])\n",
    "                            y = tf.nn.relu(y)\n",
    "                            y = tf.nn.conv2d(y,w,[1,1,1,1],\"SAME\",data_format=self.data_format)\n",
    "                            y = batch_norm(y,is_training= is_training, data_format=self.data_format)\n",
    "                    else:\n",
    "                        \n",
    "                        if y_stride > 1:\n",
    "                            assert y_stride == 2\n",
    "                            y = self._factorized_reduction(y, out_filters, y_stride, is_training = is_training)\n",
    "                        inp_c = self._get_C(y)\n",
    "                        if inp_c != out_filters:\n",
    "                            w = create_weight(\"w\",[1,1,inp_c,out_filters])\n",
    "                            y = tf.nn.relu(y)\n",
    "                            y = tf.nn.conv2d(y,w,[1,1,1,1],\"SAME\",data_format=self.data_format)\n",
    "                            y = batch_norm(y,is_training= is_training, data_format=self.data_format)\n",
    "                    if (y_op in [0,1,2,3] and self.drop_path_keep_prob is not None and is_training):\n",
    "                        y = self._apply_drop_path(y,self.drop_path_keep_prob)\n",
    "                \n",
    "                out = x + y\n",
    "                layers.append(out)\n",
    "        \n",
    "        out = self._fixed_combine(layers, used, out_filters, is_training, normal_or_reduction_cell)\n",
    "        return out\n",
    "            \n",
    "                    \n",
    "    \n",
    "    def _enas_cell(self, x, curr_cell, pre_cell, op_id, out_filters):\n",
    "        \n",
    "        num_possible_inputs = curr_cell + 2\n",
    "        \n",
    "        with tf.variable_scope(\"avg_pool\"):\n",
    "            avg_pool = tf.layers.average_pooling2d(x,[3,3],[1,1],\"SAME\",data_format=self.actual_data_format)\n",
    "            avg_pool_c = self._get_C(avg_pool)\n",
    "            if avg_pool_c != out_filters:\n",
    "                with tf.variable_scope(\"conv\"):\n",
    "                    w = create_weight(\"w\",[num_possible_inputs, avg_pool_c * out_filters])\n",
    "                    w = w[prev_cell]\n",
    "                    w = tf.reshape(w, [1,1,avg_pool_c, out_filters])\n",
    "                    avg_pool = tf.nn.relu(avg_pool)\n",
    "                    avg_pool = tf.nn.conv2d(avg_pool, w, strides=[1,1,1,1], padding=\"SAME\", data_format=self.data_format)\n",
    "                    avg_pool = batch_norm(avg_pool, is_training=True, data_format=self.data_format)\n",
    "        with tf.variable_scope(\"max_pool\"):\n",
    "            max_pool = tf.layers.max_pooling2d(x,[3,3],[1,1],\"SAME\",data_format=self.actual_data_format)\n",
    "            max_pool_c = self._get_C(max_pool)\n",
    "            if max_pool_c != out_filters:\n",
    "                with tf.variable_scope(\"conv\"):\n",
    "                    w = create_weight(\"w\", [num_possible_inputs, max_pool_c*out_filters])\n",
    "                    w = w[prev_cell]\n",
    "                    w = tf.reshape(w,[1,1,max_pool_c,out_filters])\n",
    "                    max_pool = tf.nn.relu(max_pool)\n",
    "                    max_pool = tf.nn.conv2d(max_pool,w,strides=[1,1,1,1],padding=\"SAME\",data_format=self.data_format)\n",
    "                    max_pool = batch_norm(max_pool,is_training=True,data_format=self.data_format)\n",
    "        \n",
    "        x_c = self._get_C(x)\n",
    "        if x_c != out_filters:\n",
    "            with tf.variable_scope(\"x_conv\"):\n",
    "                w = create_weight(w,[num_possible_inputs, x_c*out_filters])\n",
    "                w = w[prev_cell]\n",
    "                w = tf.reshape(w,[1,1,x_c,out_filters])\n",
    "                x = tf.nn.relu(x)\n",
    "                x = tf.nn.conv2d(x,w,strides=[1,1,1,1],padding=\"SAME\",data_format=self.data_format)\n",
    "                x = batch_norm(x,is_training=True,data_format=self.data_format)\n",
    "                \n",
    "        out = [\n",
    "            self._enas_conv(x, curr_cell, prev_cell, 3, out_filters),\n",
    "            self._enas_conv(x, curr_cell, prev_cell, 5, out_filters),\n",
    "            avg_pool,\n",
    "            max_pool,\n",
    "            x\n",
    "            \n",
    "        ]\n",
    "        \n",
    "        out = tf.stack(out, axis=0)\n",
    "        out = out[op_id,:,:,:,:]\n",
    "        return out\n",
    "     \n",
    "    def _enas_conv(self, x, curr_cell, prev_cell, filter_size, out_filters, stack_conv=2):\n",
    "        '''\n",
    "        performs an enas conv specified by the relevant parameters\n",
    "        \n",
    "        stack_conv means how many conv layers should be added. The default is 2, not copy cell\n",
    "        '''\n",
    "        \n",
    "        with tf.variable_scope(\"conv_{0}x{0}\".format(filter_size)):\n",
    "            num_possible_inputs = curr_cell + 2\n",
    "            for conv_id in range(stack_conv):\n",
    "                with tf.variable_scope(\"stack_{0}\".format(conv_id)):\n",
    "                    inp_c = self._get_C(x)\n",
    "                    w_depthwise = create_weight(\"w_depth\", [num_possible_inputs, filter_size*filter_size*inp_c])\n",
    "                    w_depthwise = w_depthwise[prev_cell,:]\n",
    "                    w_depthwise = tf.reshape(w_depthwise,[filter_size,filter_size,inp_c,1])\n",
    "                    \n",
    "                    w_pointwise = create_weight(\"w_point\", [num_possible_inputs, inp_c * out_filters])\n",
    "                    w_pointwise = w_pointwise[prev_cell,:]\n",
    "                    w_pointwise = tf.reshape(w_pointwise, [1,1,inp_c,out_filters])\n",
    "                    with tf.variable_scope(\"bn\"):\n",
    "                        zero_init = tf.initializers.zeros(dtype=tf.float32)\n",
    "                        one_init = tf.initializers.ones(dtype=tf.float32)\n",
    "                        offset = create_weight(\"offset\", [num_possible_inputs, out_filters],initializer=zero_init)\n",
    "                        scale = create_weight(\"scale\", [num_possible_inputs, out_filters],initializer=one_init)\n",
    "                        offset = offset[prev_cell]\n",
    "                        scale = scale[prev_cell]\n",
    "                    x = tf.nn.relu(x)\n",
    "                    x = tf.nn.separable_conv2d(x,depthwise_filter=w_depthwise,pointwise_filter=w_pointwise,strides=[1,1,1,1],padding=\"SAME\",data_format=self.data_format)\n",
    "                    \n",
    "                    \n",
    "                    x, _,_ = tf.nn.fused_batch_norm(x,scale=scale,offset=offset,epsilon=1e-5,data_format=self.data_format,is_training=True)\n",
    "                    \n",
    "        return x\n",
    "    \n",
    "    def _enas_layer(self, layer_id, pre_layers, arc, out_filters):\n",
    "        \n",
    "        assert len(prev_layers) == 2, \"need exactly 2 inputs\"\n",
    "        layers = [prev_layers[0], prev_layers[1]]\n",
    "        layers = self._maybe_calibrate_size(layers, out_filters, is_training = True)\n",
    "        used = []\n",
    "        \n",
    "        for cell_id in range(self.num_cells):\n",
    "            prev_layers = tf.stack(layers, axis=0)\n",
    "            with tf.variable_scope(\"cell_{0}\".format(cell_id)):\n",
    "                with tf.variable_scope(\"x\"):\n",
    "                    x_id = arc[4*cell_id]\n",
    "                    x_op = arc[4*cell_id+1]\n",
    "                    x = prev_layers[x_id,:,:,:,:]\n",
    "                    x = self._enas_cell(x, cell_id, x_id, x_op, out_filters)\n",
    "                    x_used = tf.one_hot(x_id, depth=self.num_cells+2, dtype=tf.int32)\n",
    "                \n",
    "                with tf.variable_scope(\"y\"):\n",
    "                    y_id = arc[4*cell_id+2]\n",
    "                    y_op = arc[4*cell_id+3]\n",
    "                    y = prev_layers[y_id,:,:,:,:]\n",
    "                    y = self._enas_cell(y, cell_id, y_id, y_op, out_filters)\n",
    "                    y_used = tf.one_hot(y_id, depth=self.num_cells+2, dtype=tf.int32)\n",
    "                \n",
    "                out = x + y\n",
    "                used.extended([x_used, y_used])\n",
    "                layers.append(out)\n",
    "        \n",
    "        used = tf.add_n(used)\n",
    "        indices = tf.where(tf.equal(used,0))\n",
    "        indices = tf.to_int32(indices)\n",
    "        indices = tf.reshape(indices, [-1])\n",
    "        num_outs = tf.size(indices)\n",
    "        out = tf.stack(layers,axis=0)\n",
    "        out = tf.gather(out,indices,axis=0)\n",
    "        \n",
    "        inp = prev_layers[0]\n",
    "        \n",
    "        if self.data_format == \"NHWC\":\n",
    "            N = tf.shape(inp)[0]\n",
    "            H = tf.shape(inp)[1]\n",
    "            W = tf.shape(inp)[2]\n",
    "            C = tf.shape(inp)[3]\n",
    "            out = tf.transpose(out, [1,2,3,0,4])\n",
    "            out = tf.reshape(out, [N,H,W, num_outs * out_filters])\n",
    "        elif self.data_format == \"HCHW\":\n",
    "            N = tf.shape(inp)[0]\n",
    "            C = tf.shape(inp)[1]\n",
    "            H = tf.shape(inp)[2]\n",
    "            W = tf.shape(inp)[3]\n",
    "            out = tf.transpose(out, [1,0,2,3,4])\n",
    "            out = tf.reshape(out,[N, num_outs * out_filters, H, W])\n",
    "        else:\n",
    "            raise ValueError(\"Unknown data_format '{0}'\".format(self.data_format))\n",
    "        \n",
    "        with tf.variable_scope(\"final_conv\"):\n",
    "            w = create_weight(\"w\", [self.num_cells+2, out_filters * out_filters ])\n",
    "            w = tf.gather(w,indices,axis = 0)\n",
    "            w = tf.reshape(w, [1,1,num_outs*out_filters, out_filters])\n",
    "            out = tf.nn.relu(out)\n",
    "            out = tf.nn.conv2d(out, w, strides=[1,1,1,1],padding=\"SAME\",data_format=self.data_format)\n",
    "            out = batch_norm(out,is_training=True, data_format=self.data_format)\n",
    "        out = tf.reshape(out, tf.shape(prev_layers[0]))\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def _build_train(self):\n",
    "        tf.logging.info(\"-\" * 80)\n",
    "        tf.logging.info(\"Build train graph\")\n",
    "        logits = self._model(self.x_train, is_training = True, reuse = tf.AUTO_REUSE)\n",
    "        log_probs = tf.nn.sparse_softmax_cross_entropy_with_logits(logits = logits, labels=self.y_train)\n",
    "        self.loss = tf.reduce_mean(log_probs)\n",
    "        \n",
    "        if self.use_aux_heads:\n",
    "            log_probs = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=self.aux_logits, labels=self.y_train)\n",
    "            self.aux_loss = tf.reduce_mean(log_probs)\n",
    "            train_loss = self.loss + 0.4 * self.aux_loss\n",
    "        else:\n",
    "            train_loss = self.loss\n",
    "        \n",
    "        self.train_preds = tf.argmax(logits, axis=1)\n",
    "        self.train_preds = tf.to_int32(self.train_preds)\n",
    "        self.train_acc = tf.equal(self.train_preds,self.y_train)\n",
    "        self.train_acc = tf.to_int32(self.train_acc)\n",
    "        self.train_acc = tf.reduce_sum(self.train_acc)\n",
    "        \n",
    "        tf_variables = [var for var in tf.trainable_variables() if var.name.startwith(self.name)]\n",
    "        self.num_vars = count_model_params(tf_variables)\n",
    "        tf.logging.info(\"Model has {0} params\".format(self.num_vars))\n",
    "        \n",
    "        with tf.variable_scope(self.name, reuse=tf.AUTO_REUSE):\n",
    "            self.train_op, self.lr, self.grad_norm, self.optimizer = get_train_ops(train_loss,\n",
    "                                                                                   tf_variables,\n",
    "                                                                                   self.global_step,\n",
    "                                                                                   clip_mode = self.clip_mode,\n",
    "                                                                                   grad_bound = self.grad_bound,\n",
    "                                                                                   l2_reg = self.l2_reg,\n",
    "                                                                                   lr_init = self.lr_init,\n",
    "                                                                                   lr_dec_start = self.lr_dec_start,\n",
    "                                                                                   lr_dec_every = self.lr_dec_every,\n",
    "                                                                                   lr_dec_rate = self.lr_dec_rate,\n",
    "                                                                                   lr_cosine = self.lr_cosine,\n",
    "                                                                                   lr_max = self.lr_max,\n",
    "                                                                                   lr_min = self.lr_min,\n",
    "                                                                                   lr_T_0 = self.lr_T_0,\n",
    "                                                                                   lr_T_mul = self.lr_T_mul,\n",
    "                                                                                   num_train_batches = self.num_train_batches,\n",
    "                                                                                   optim_algo = self.optim_algo,\n",
    "                                                                                   sync_replicas = self.sync_replicas,\n",
    "                                                                                   num_aggregate = self.num_aggregate,\n",
    "                                                                                   num_replicas = self.num_replicas)\n",
    "            \n",
    "    \n",
    "    def _build_valid(self):\n",
    "        if self.x_valid is not None\n",
    "            tf.logging.info(\"-\" * 80)\n",
    "            tf.logging.info(\"Build valid graph\")\n",
    "            logits = self._model(self.x_valid, False, reuse = tf.AUTO_REUSE)\n",
    "            self.valid_preds = tf.argmax(logits,axis = 1)\n",
    "            self.valid_preds = tf.to_int32(self.valid_preds)\n",
    "            self.valid_acc = tf.equal(self.valid_preds,self.y_valid)\n",
    "            self.valid_acc = tf.to_int32(self.valid_acc)\n",
    "            self.valid_acc = tf.reduce_sum(self.valid_acc)\n",
    "    \n",
    "    def _build_test(self):\n",
    "        tf.logging.info(\"-\" * 80)\n",
    "        tf.logging.info(\"Build test graph\")\n",
    "        logits = self._model(self.x_test, False, reuse = tf.AUTO_REUSE)\n",
    "        self.test_preds = tf.argmax(logits, axis=1)\n",
    "        self.test_preds = tf.to_int32(self.test_preds)\n",
    "        self.test_acc = tf.equal(self.test_preds, self.y_test)\n",
    "        self.test_acc = tf.to_int32(self.test_acc)\n",
    "        self.test_acc = tf.reduce_sum(self.test_acc)\n",
    "    \n",
    "    def connect_controller(self, arch_pool=None, arch_pool_prob=None):\n",
    "        if self.fixed_arc is None:\n",
    "            self.normal_arc, self.reduce_arc = sample_arch_from_pool(arch_pool, arch_pool_prob)\n",
    "        else:\n",
    "            fixed_arc = np.array([int(x) for x in self.fixed_arc.split(\" \") if x])\n",
    "            self.normal_arc = fixed_arc[:4 * self.num_cells]\n",
    "            self.reduce_arc = fixed_arc[4 * self.num_cells:]\n",
    "        \n",
    "        self._build_train()\n",
    "        self._build_valid()\n",
    "        self._build_test()\n",
    "            \n",
    "                    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "输入训练数据和child_params 用来返回训练时所需要的参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ops(images, labels, params):\n",
    "    child_model = Model(\n",
    "        images,\n",
    "        labels,\n",
    "        use_aux_heads = params['use_aux_heads'],\n",
    "        cutout_size = params['cutout_size'],\n",
    "        num_layers = params['num_layers'],\n",
    "        num_cells = params['num_cells'],\n",
    "        fixed_arc = params['fixed_arc'],\n",
    "        out_filters_scale = params['out_filters_scale'],\n",
    "        out_filters = params['out_filters'],\n",
    "        keep_prob = params['keep_prob'],\n",
    "        drop_path_keep_prob = params['drop_path_keep_prob'],\n",
    "        num_epochs = params['num_epochs'],\n",
    "        l2_reg = params['l2_reg'],\n",
    "        data_format = params['data_format'],\n",
    "        batch_size = params['batch_size'],\n",
    "        eval_batch_size = params['eval_batch_size'],\n",
    "        clip_mode = 'norm',\n",
    "        grad_bound = params['grad_bound'],\n",
    "        lr_init = params['lr'],\n",
    "        lr_dec_every = params['lr_dec_every'],\n",
    "        lr_dec_rate = params['lr_dec_rate'],\n",
    "        lr_cosine = params['lr_cosine'],\n",
    "        lr_max = params['lr_max'],\n",
    "        lr_min = params['lr_min'],\n",
    "        lr_T_0 = params['lr_T_0'],\n",
    "        lr_T_mul = params['lr_T_mul'],\n",
    "        optim_algo = 'momentum',\n",
    "        sync_replicas = params['sync_replicas'],\n",
    "        num_aggregate = params['num_aggregate'],\n",
    "        num_replicas = params['num_replicas'],   \n",
    "    )\n",
    "    if params['fixed_arc'] is None:\n",
    "        child_model.connect_controller(params['arch_pool'], params['arch_pool_prob'])\n",
    "    else:\n",
    "        child_model.connect_controller(None, None)\n",
    "    \n",
    "    ops = {\n",
    "        'global_step': child_model.global_step,\n",
    "        'loss': child_model.loss,\n",
    "        'train_op': child_model.train_op,\n",
    "        'lr': child_model.lr,\n",
    "        'grad_norm': child_model.grad_norm,\n",
    "        'train_acc': child_model.train_acc,\n",
    "        'optimizer': child_model.optimizer,\n",
    "        'num_train_batches': child_model.num_train_batches,\n",
    "        'eval_every': child_model.num_train_batches * params['eval_every_epochs'],\n",
    "        'eval_func': child_model.eval_once\n",
    "        \n",
    "    }\n",
    "    return ops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "根据传进来的参数训练，传进来的params中arch_pool一项包含了所给定的架构，这个函数就是针对这些给定的架构，开始普通参数的训练\n",
    "\n",
    "get_ops应该是用来返回训练时所需要的参数，暂时未实现\n",
    "\n",
    "tf.train.Saver用于模型图中变量的保存和恢复\n",
    "\n",
    "tf.train.CheckpointSaverHook构造一个hook对象，用来保存东西，其中save_steps表示每隔多少步保存一下，在这里指的是，训练一次全部数据后保存一下\n",
    "\n",
    "sync_replicas其实是分布式训练中所需要的参数，会配合分布式的optimizer来使用，make_session_run_hook表示Creates a hook to handle SyncReplicasHook ops such as initialization. 暂时不是很能理解其中的意思\n",
    "\n",
    "num_aggregate参数也同样出自分布式训练中，应该表示多少个子节点，因此接受完这些子节点之后，取这些梯度的平均，才做一次更新，global_step才会+1，这就是为啥actual_step计算的时候，需要global_step * num_aggregate\n",
    "\n",
    "epoch = actual_step // child_ops['num_train_batches']用来计算总共训练了多少轮了\n",
    "\n",
    "child_ops['eval_every']用于整个循环的终止，可能表达的意思就是训练多少轮之后就可以进行评价了，大概是这么个意思,用来控制的是每个子架构的训练轮数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(params):\n",
    "    g = tf.Graph()\n",
    "    with g.as_default():\n",
    "        images, labels = read_data(params['data_dir'])\n",
    "        child_ops = get_ops(images, labels, params)\n",
    "        saver = tf.train.Saver(max_to_keep=100)\n",
    "        checkpoint_saver_hook = tf.train.CheckpointSaverHook(params['model_dir'], save_steps=child_ops['num_train_batches'], saver=saver)\n",
    "        \n",
    "        hooks = [checkpoint_saver_hook]\n",
    "        if params['sync_replicas']:\n",
    "            sync_replicas_hook = child_ops['optimizer'].make_session_run_hook(True)\n",
    "            hooks.append(sync_replicas_hook)\n",
    "        \n",
    "        tf.logging.info('-' * 80)\n",
    "        tf.logging.info('Starting session')\n",
    "        config = tf.ConfigProto(allow_soft_placement=True)\n",
    "        with tf.train.SingularMonitoredSession(config=config, hooks=hooks, checkpoint_dir=params['model_dir']) as sess:\n",
    "            start_time = time.time()\n",
    "            while True:\n",
    "                run_ops = [\n",
    "                    child_ops['loss'],\n",
    "                    child_ops['lr'],\n",
    "                    child_ops['grad_norm'],\n",
    "                    child_ops['train_acc'],\n",
    "                    child_ops['train_op'],\n",
    "                ]\n",
    "                loss, lr, gn, tr_acc, _ = sess.run(run_ops)\n",
    "                global_step = sess.run(child_ops['global_step'])\n",
    "                \n",
    "                if params['sync_replicas']:\n",
    "                    actual_step = global_step * params['num_aggregate']\n",
    "                else:\n",
    "                    actual_step = global_step\n",
    "                \n",
    "                epoch = actual_step // child_ops['num_train_batches']\n",
    "                curr_time = time.time()\n",
    "                \n",
    "                if global_step % 50 == 0:\n",
    "                    log_string = ''\n",
    "                    log_string += 'epoch={:<6d}'.format(epoch)\n",
    "                    log_string += 'ch_step={:<6d}'.format(global_step)\n",
    "                    log_string += ' loss={:<8.6f}'.format(loss)\n",
    "                    log_string += ' lr={:<8.4f}'.format(lr)\n",
    "                    log_string += ' |g|={:<8.4f}'.format(gn)\n",
    "                    log_string += ' tr_acc={:<3d}/{:>3d}'.format(tr_acc, params['batch_size'])\n",
    "                    log_string += ' mins={:<10.2f}'.format(float(curr_time-start_time)/60)\n",
    "                    tf.logging.info(log_string)\n",
    "                \n",
    "                if actual_step % child_ops['eval_every'] == 0:\n",
    "                    return epoch\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "输入图像，标签以及参数集合params，其中params里面包含了一些架构\n",
    "\n",
    "其中按照fixed_arc有无来进行操作，暂时不知道fixed_arc是干什么用的，字面意思固定架构，可能在后期的时候，需要指定架构之类的\n",
    "\n",
    "返回的ops里面就只有一项valid-acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_valid_ops(images,labels,params):\n",
    "    child_model = Model(\n",
    "        images,\n",
    "        labels,\n",
    "        use_aux_heads = params[\"use_aux_heads\"],\n",
    "        cutout_size = params[\"cutout_size\"],\n",
    "        num_layers = params[\"num_layers\"],\n",
    "        num_cells = params[\"num_cells\"],\n",
    "        fixed_arc = params[\"fixed_arc\"],\n",
    "        out_filters_scale = params[\"out_filters_scale\"],\n",
    "        out_filters = params[\"out_filters\"],\n",
    "        keep_prob = params[\"keep_prob\"],\n",
    "        drop_path_keep_prob = params[\"drop_path_keep_prob\"],\n",
    "        num_epochs = params[\"num_epochs\"],\n",
    "        l2_reg = params[\"l2_reg\"],\n",
    "        data_format = params[\"data_format\"],\n",
    "        batch_size = params[\"batch_size\"],\n",
    "        eval_batch_size = params[\"eval_batch_size\"],\n",
    "        clip_mode = \"norm\",\n",
    "        grad_bound = params[\"grad_bound\"],\n",
    "        lr_init = params[\"lr\"],\n",
    "        lr_dec_every = params[\"lr_dec_every\"],\n",
    "        lr_dec_rate = params[\"lr_dec_rate\"],\n",
    "        lr_cosine = params[\"lr_cosine\"],\n",
    "        lr_max = params[\"lr_max\"],\n",
    "        lr_min = params[\"lr_min\"],\n",
    "        lr_T_0 = params[\"lr_T_0\"],\n",
    "        lr_T_mul = params[\"lr_T_mul\"],\n",
    "        optim_algo = \"momentum\",\n",
    "        sync_replicas = params[\"sync_replicas\"],\n",
    "        num_aggregate = params[\"num_aggregate\"],\n",
    "        num_replicas = params[\"num_replicas\"],\n",
    "    )\n",
    "    \n",
    "    if params[\"fixed_arc\"] is None:\n",
    "        arch_pool = tf.convert_to_tensor(params[\"arch_pool\"], dtype = tf.int32)\n",
    "        arch_pool = tf.data_Dataset.from_tensor_slices(arch_pool)\n",
    "        arch_pool = arch_pool.map(lambda x: (x[0],x[1]))\n",
    "        iterator = arch_pool.make_one_shot_iterator()\n",
    "        conv_arch, reduc_arch = iterator.get_next()\n",
    "        child_model.normal_arc = conv_arch\n",
    "        child_model.reduce_arc = reduc_arch\n",
    "        child_model._build_valid()\n",
    "        valid_acc = tf.cast(child_model.valid_acc / child_model.eval_batch_size, tf.float32)\n",
    "        ops = {\n",
    "            \"valid_acc\": valid_acc,\n",
    "        }\n",
    "    else:\n",
    "        child_model.connect_controller(None,None)\n",
    "        ops = {\n",
    "            \"valid_acc\": child_model.valid_acc / child_model.eval_batch_size,\n",
    "        }\n",
    "    \n",
    "    return ops\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "输入一些架构，输出一个列表，列表里每个元素是各个架构对应的在验证集上的精确度\n",
    "\n",
    "    tf.configProto用来配置计算session的GPU和CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid(params):\n",
    "    g = tf.Graph()\n",
    "    with g.as_default():\n",
    "        images,labels = read_data(params['data_dir'])\n",
    "        child_ops = get_valid_ops(images,labels,params)\n",
    "        tf.logging.info(\"-\" * 80)\n",
    "        tf.logging.info(\"Starting session\")\n",
    "        config = tf.ConfigProto(allow_soft_placement=True)\n",
    "        N = len(params['arch_pool'])\n",
    "        valid_acc_list = []\n",
    "        with tf.train.SingularMonitoredSession(config=config, checkpoint_dir=params['model_dir']) as sess:\n",
    "            start_time = time.time()\n",
    "            for i in range(N):\n",
    "                run_ops = [ child_ops[\"valid_acc\"],]\n",
    "                valid_acc,  = sess.run(run_ops)\n",
    "                valid_acc_list.append(float(valid_acc))\n",
    "            curr_time = time.time()\n",
    "            log_string = \"\"\n",
    "            log_string += \" valid_acc={:<6.6f}\".format(np.mean(valid_acc_list))\n",
    "            log_string += ' min={:<10.2f}'.format(float(curr_time-start_time) / 60)\n",
    "            tf.logging.info(log_string)\n",
    "        return valid_acc_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
